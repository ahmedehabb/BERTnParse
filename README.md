# Bert-n-Parse

## ğŸ§  Dependency Graph Extractor

This script preprocesses Arabic sentences from the [CAMeL-Lab/BAREC-Shared-Task-2025-sent](https://huggingface.co/datasets/CAMeL-Lab/BAREC-Shared-Task-2025-sent) dataset and extracts rich linguistic features for graph-based modeling. It leverages the [CamelParser v2.0](https://github.com/CAMeL-Lab/camel_parser) for syntactic analysis and outputs structured data for downstream machine learning or analysis tasks.

### ğŸ” Extracted Features
- **Morphological features**
- **Part-of-speech (POS) tags**
- **Universal Dependencies (UD)**
- **Dependency graph structure (head/deprel info)**

---

### ğŸš€ How to Run

Inside the `camel_parser/` directory:

```bash
python dependency_graph_extractor.py <Split>
```

Replace `<Split>` with one of:

- `train`
- `validation`
- `test`

Example:

```bash
python dependency_graph_extractor.py train
```

This will:
1. Load the dataset split (e.g., `train`).
2. Normalize and clean the Arabic sentences.
3. Split the data into manageable chunks.
4. Use `text_to_conll_cli.py` to parse each chunk using CamelParser.
5. Extract and save:
   - Morphological and syntactic features
   - Dependency relations
   - Sentence-level statistics

ğŸ’¡ Alternatively, you can run the full pipeline using the provided notebook:  
ğŸ““ **`Camel_parser.ipynb`** â€“ includes all steps from preprocessing to dependency graph extraction in one place.

---

### ğŸ“¦ Output Files (Saved in `./splits_<Split>/`)

| File Name                       | Description                                                  |
|--------------------------------|--------------------------------------------------------------|
| `sentences_i_j.txt`            | Cleaned input sentences for parser chunk `i_j`               |
| `output_i_j.txt`               | Parser output (CoNLL format) for chunk `i_j`                 |
| `all_parsed_blocks_<Split>.txt`| All CoNLL-formatted parses merged into one file             |
| `features_list_<Split>.json`   | Extracted statistics and metadata per sentence               |
| `pos_tags_dict_<Split>.json`   | Mapping from sentence â†’ list of POS tags                     |
| `morph_features_dict_<Split>.json` | Mapping from sentence â†’ morphological features           |
| `dep_graph_<Split>.json`       | Dependency graphs as JSON-serializable structures            |

---

### ğŸ“ Notes

- All sentences are **de-diacritized** using `pyarabic.araby.strip_diacritics`.
- Duplicate sentences are **removed before processing**.
- Parsing is done via **CamelParser2.0** using its CLI interface.
- Each output file is keyed by the **cleaned sentence string** to ensure consistent merging.

### ğŸ”— Dependency Parsing Tool
Dependency parsing in this project was performed using **CamelParser2.0: A State-of-the-Art Dependency Parser for Arabic** (Elshabrawy et al., 2023).

## Model Training: BERT + Graph Neural Networks

Once the linguistic features and dependency graphs have been extracted and saved (see [Dependency Graph Extractor](#dependency-graph-extractor)), the next stage involves training a **graph-based readability classifier** on top of BERT embeddings using PyTorch Geometric.

All training logic is implemented in [`training/train.py`](./training/train.py).

---

### ğŸ§  Model Overview

The model architecture consists of:
- **BERT encoder** (`aubmindlab/bert-base-arabertv02`) to generate contextual token embeddings.
- **Graph Neural Network (GNN)** that operates over dependency graphs, encoding syntactic structure.
- **CORAL-based ordinal classifier** for predicting Arabic readability levels (1â€“19).

You can choose between two GNN architectures:
- `ImprovedSimpleEdgeGNN` â€“ simple but effective edge-based model.
- `StrongerEdgeGNN` â€“ deeper and more expressive, with multi-head attention and dropout.

---

### ğŸ”„ End-to-End Pipeline

The `train.py` script performs the following steps:

#### 1. **Load & Preprocess the Dataset**
- Loads all splits from the [CAMeL-Lab/BAREC-Shared-Task-2025-sent](https://huggingface.co/datasets/CAMeL-Lab/BAREC-Shared-Task-2025-sent) dataset using Hugging Face Datasets.
- Normalizes Arabic text (removes diacritics using `pyarabic`).
- Ensures proper spacing after punctuation for better dependency parsing.
- Adjusts labels to be 0-indexed (`Readability_Level_19 - 1`).

#### 2. **Attach Dependency Features**
- Each sentence is augmented with:
  - Dependency labels
  - POS tags
  - Head indices
- This data is merged with previous CoNLL-style parses using string-based matching.

#### 3. **Build Graph Representations**
- Dependency graphs are converted into `torch_geometric.data.Data` objects.
- Edge labels use `dep2id`; node features use `pos2id`.
- All graphs are padded so they have equal-length node feature vectors.

#### 4. **Cache Graphs**
- Graphs are built once per split and cached under a `cache/` folder as `.pt` files (e.g., `graph_list_train.pt`).
- Re-running the script reuses cached files unless deleted or modified.

#### 5. **Model Training**
- Trains the GNN over BERT embeddings using CORAL loss for ordinal classification.
- Evaluation is done on the internal test set each epoch.
- Best models (based on accuracy and Quadratic Weighted Kappa) are saved:
  - `model_best_acc.pt`
  - `model_best_qwk.pt`
  - `last_model.pt`

#### 6. **Final Evaluation**
- After training, the best model is evaluated on the **official blind test split**.
- Predictions are saved to a file using `output_to_file`.

---

### ğŸ› ï¸ How to Run

From the root of the project:

```bash
python training/train.py
```

Make sure you've already:
- Installed required dependencies (see below).
- Parsed all splits using the [`camel_parser`](./camel_parser) pipeline.

ğŸ’¡ Alternatively, you can run the full end-to-end pipeline (from data loading to final evaluation) using the provided notebook:  
ğŸ““ **`barec-model.ipynb`** â€“ mirrors all steps in `train.py` in one place for easy experimentation and interactive debugging.

---

### ğŸ“¦ Dependencies

You can install all dependencies using:

```bash
pip install transformers pyarabic regex coral-pytorch camel-tools torch_geometric
pip install -U datasets huggingface_hub
```

---

### ğŸ“ Directory Structure (Relevant Files)

```
.
â”œâ”€â”€ camel_parser/
â”‚   â””â”€â”€ .... 
â”‚   â””â”€â”€ dependency_graph_extractor.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py  â† Full model training pipeline
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ graph_utils.py
â”‚   â”œâ”€â”€ attach_morph_features.py
â”‚   â”œâ”€â”€ attach_dep_features.py
â”‚   â””â”€â”€ dependency_relations.py
â”‚
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ graph_list_{split}.pt  â† Cached graph data per dataset split
```

---

### ğŸ“ Notes

- This setup is optimized for experimentation and reuse.
- You can skip morph features if you prefer a cleaner dependency-based model.
- The GNN expects properly padded graphs â€” this is handled automatically via `pad_pos_tags`.

---
### ğŸ“š Citation

If you find the Bert-n-Parse useful in your research, please cite: 

> Your Name  
> *Title of Your Work*  
> Conference/Journal, Year  
> DOI or URL (if available)
