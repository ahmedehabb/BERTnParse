{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:26.615684Z",
     "iopub.status.busy": "2025-07-21T19:34:26.615487Z",
     "iopub.status.idle": "2025-07-21T19:34:33.441811Z",
     "shell.execute_reply": "2025-07-21T19:34:33.440873Z",
     "shell.execute_reply.started": "2025-07-21T19:34:26.615657Z"
    },
    "id": "jeDnPf5hdYWN",
    "outputId": "c76346b1-7aaf-47fc-fb2d-d34467634eec",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.43.4)\n",
      "Requirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
      "Requirement already satisfied: coral-pytorch in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
      "Requirement already satisfied: camel-tools in /usr/local/lib/python3.11/dist-packages (1.5.6)\n",
      "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from coral-pytorch) (75.2.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.0.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.6.2)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from camel-tools) (5.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.15.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.2.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.3.8)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.6.0+cu124)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.8.1)\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.14.1)\n",
      "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.20.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.9.0)\n",
      "Requirement already satisfied: muddler in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.1.3)\n",
      "Requirement already satisfied: camel-kenlm>=2025.4.8 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2025.4.8)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.13)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->camel-tools) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (3.6.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers pyarabic regex coral-pytorch camel-tools torch_geometric\n",
    "!pip install -U datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:33.444346Z",
     "iopub.status.busy": "2025-07-21T19:34:33.444127Z",
     "iopub.status.idle": "2025-07-21T19:34:33.448692Z",
     "shell.execute_reply": "2025-07-21T19:34:33.447636Z",
     "shell.execute_reply.started": "2025-07-21T19:34:33.444325Z"
    },
    "id": "XNygS-Jg2trH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:33.450029Z",
     "iopub.status.busy": "2025-07-21T19:34:33.449744Z",
     "iopub.status.idle": "2025-07-21T19:34:42.024884Z",
     "shell.execute_reply": "2025-07-21T19:34:42.024309Z",
     "shell.execute_reply.started": "2025-07-21T19:34:33.450007Z"
    },
    "id": "nMUNZGVPxJPV",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 19:34:37.758203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753126477.781134    1328 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753126477.788508    1328 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from functools import partial\n",
    "import pyarabic.araby as araby\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:42.026321Z",
     "iopub.status.busy": "2025-07-21T19:34:42.025728Z",
     "iopub.status.idle": "2025-07-21T19:34:42.032101Z",
     "shell.execute_reply": "2025-07-21T19:34:42.031456Z",
     "shell.execute_reply.started": "2025-07-21T19:34:42.026291Z"
    },
    "id": "FJ7-WvSf3QDg",
    "outputId": "cb44c51a-31f7-4995-c2a9-a952437c0815",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdZZWgz_13Rq"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:42.033251Z",
     "iopub.status.busy": "2025-07-21T19:34:42.033072Z",
     "iopub.status.idle": "2025-07-21T19:34:53.902793Z",
     "shell.execute_reply": "2025-07-21T19:34:53.902000Z",
     "shell.execute_reply.started": "2025-07-21T19:34:42.033237Z"
    },
    "id": "d3TGT9L8xJPW",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Readability_Level</th>\n",
       "      <th>Readability_Level_19</th>\n",
       "      <th>Readability_Level_7</th>\n",
       "      <th>Readability_Level_5</th>\n",
       "      <th>Readability_Level_3</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Document</th>\n",
       "      <th>Source</th>\n",
       "      <th>Book</th>\n",
       "      <th>Author</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Text_Class</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10100290001</td>\n",
       "      <td>مجلة كل الأولاد وكل البنات</td>\n",
       "      <td>5</td>\n",
       "      <td>7-zay</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A2</td>\n",
       "      <td>BAREC_Majed_0413_1987_001.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10100290002</td>\n",
       "      <td>ماجد</td>\n",
       "      <td>1</td>\n",
       "      <td>1-alif</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A2</td>\n",
       "      <td>BAREC_Majed_0413_1987_001.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10100290003</td>\n",
       "      <td>الأربعاء 21 يناير 1987</td>\n",
       "      <td>4</td>\n",
       "      <td>8-Ha</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A3</td>\n",
       "      <td>BAREC_Majed_0413_1987_001.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10100290004</td>\n",
       "      <td>الموافق 21 جمادى الأول 1407هــ</td>\n",
       "      <td>6</td>\n",
       "      <td>7-zay</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A3</td>\n",
       "      <td>BAREC_Majed_0413_1987_001.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10100290005</td>\n",
       "      <td>السنة الثامنة</td>\n",
       "      <td>2</td>\n",
       "      <td>5-ha</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A4</td>\n",
       "      <td>BAREC_Majed_0413_1987_001.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10100300074</td>\n",
       "      <td>أحمد بالقاسم خليفة</td>\n",
       "      <td>3</td>\n",
       "      <td>3-jim</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A2</td>\n",
       "      <td>BAREC_Majed_0413_1987_002.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>10100300075</td>\n",
       "      <td>المدينة المنورة – السعودية</td>\n",
       "      <td>4</td>\n",
       "      <td>11-kaf</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "      <td>BAREC_Majed_0413_1987_002.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10100300076</td>\n",
       "      <td>الحصان العربي.. قوي.. رشيق.. جميل.. سريع.</td>\n",
       "      <td>15</td>\n",
       "      <td>7-zay</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A4</td>\n",
       "      <td>BAREC_Majed_0413_1987_002.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10100300077</td>\n",
       "      <td>الدينار</td>\n",
       "      <td>1</td>\n",
       "      <td>11-kaf</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "      <td>BAREC_Majed_0413_1987_002.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10100300078</td>\n",
       "      <td>•كم يساوى الدينار الكويتي بالنسبة لدرهم الإمارات؟</td>\n",
       "      <td>9</td>\n",
       "      <td>7-zay</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A5</td>\n",
       "      <td>BAREC_Majed_0413_1987_002.txt</td>\n",
       "      <td>Majed</td>\n",
       "      <td>Edition: 413</td>\n",
       "      <td>#</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                           Sentence  \\\n",
       "0   10100290001                         مجلة كل الأولاد وكل البنات   \n",
       "1   10100290002                                               ماجد   \n",
       "2   10100290003                             الأربعاء 21 يناير 1987   \n",
       "3   10100290004                     الموافق 21 جمادى الأول 1407هــ   \n",
       "4   10100290005                                      السنة الثامنة   \n",
       "..          ...                                                ...   \n",
       "95  10100300074                                 أحمد بالقاسم خليفة   \n",
       "96  10100300075                         المدينة المنورة – السعودية   \n",
       "97  10100300076          الحصان العربي.. قوي.. رشيق.. جميل.. سريع.   \n",
       "98  10100300077                                            الدينار   \n",
       "99  10100300078  •كم يساوى الدينار الكويتي بالنسبة لدرهم الإمارات؟   \n",
       "\n",
       "    Word_Count Readability_Level  Readability_Level_19  Readability_Level_7  \\\n",
       "0            5             7-zay                     7                    2   \n",
       "1            1            1-alif                     1                    1   \n",
       "2            4              8-Ha                     8                    3   \n",
       "3            6             7-zay                     7                    2   \n",
       "4            2              5-ha                     5                    2   \n",
       "..         ...               ...                   ...                  ...   \n",
       "95           3             3-jim                     3                    1   \n",
       "96           4            11-kaf                    11                    4   \n",
       "97          15             7-zay                     7                    2   \n",
       "98           1            11-kaf                    11                    4   \n",
       "99           9             7-zay                     7                    2   \n",
       "\n",
       "    Readability_Level_5  Readability_Level_3 Annotator  \\\n",
       "0                     1                    1        A2   \n",
       "1                     1                    1        A2   \n",
       "2                     2                    1        A3   \n",
       "3                     1                    1        A3   \n",
       "4                     1                    1        A4   \n",
       "..                  ...                  ...       ...   \n",
       "95                    1                    1        A2   \n",
       "96                    2                    1        A1   \n",
       "97                    1                    1        A4   \n",
       "98                    2                    1        A1   \n",
       "99                    1                    1        A5   \n",
       "\n",
       "                         Document Source          Book Author  \\\n",
       "0   BAREC_Majed_0413_1987_001.txt  Majed  Edition: 413      #   \n",
       "1   BAREC_Majed_0413_1987_001.txt  Majed  Edition: 413      #   \n",
       "2   BAREC_Majed_0413_1987_001.txt  Majed  Edition: 413      #   \n",
       "3   BAREC_Majed_0413_1987_001.txt  Majed  Edition: 413      #   \n",
       "4   BAREC_Majed_0413_1987_001.txt  Majed  Edition: 413      #   \n",
       "..                            ...    ...           ...    ...   \n",
       "95  BAREC_Majed_0413_1987_002.txt  Majed  Edition: 413      #   \n",
       "96  BAREC_Majed_0413_1987_002.txt  Majed  Edition: 413      #   \n",
       "97  BAREC_Majed_0413_1987_002.txt  Majed  Edition: 413      #   \n",
       "98  BAREC_Majed_0413_1987_002.txt  Majed  Edition: 413      #   \n",
       "99  BAREC_Majed_0413_1987_002.txt  Majed  Edition: 413      #   \n",
       "\n",
       "               Domain    Text_Class  labels  \n",
       "0   Arts & Humanities  Foundational       6  \n",
       "1   Arts & Humanities  Foundational       0  \n",
       "2   Arts & Humanities  Foundational       7  \n",
       "3   Arts & Humanities  Foundational       6  \n",
       "4   Arts & Humanities  Foundational       4  \n",
       "..                ...           ...     ...  \n",
       "95  Arts & Humanities  Foundational       2  \n",
       "96  Arts & Humanities  Foundational      10  \n",
       "97  Arts & Humanities  Foundational       6  \n",
       "98  Arts & Humanities  Foundational      10  \n",
       "99  Arts & Humanities  Foundational       6  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load sentence-level data from the \"Dev\" split\n",
    "dataset = load_dataset(\"CAMeL-Lab/BAREC-Shared-Task-2025-sent\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"CAMeL-Lab/BAREC-Shared-Task-2025-sent\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"CAMeL-Lab/BAREC-Shared-Task-2025-sent\", split=\"test\")\n",
    "\n",
    "# Load sentence-level data from the \"official_blind\" split\n",
    "# Set token to access the blind test dataset\n",
    "# read token from environment variable\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\", None)\n",
    "if token is None:\n",
    "    raise ValueError(\"Please set the HUGGINGFACE_TOKEN environment variable.\")\n",
    "blind_test_dataset = load_dataset(\"CAMeL-Lab/BAREC-Shared-Task-2025-BlindTest-sent\", token=token, split=\"test\")\n",
    "\n",
    "# Fix labels to be 0-indexed\n",
    "dataset = dataset.map(lambda x: {\"labels\": x[\"Readability_Level_19\"] - 1, \"Sentence\": araby.strip_diacritics(x[\"Sentence\"])})\n",
    "eval_dataset = eval_dataset.map(lambda x: {\"labels\": x[\"Readability_Level_19\"] - 1, \"Sentence\": araby.strip_diacritics(x[\"Sentence\"])})\n",
    "test_dataset = test_dataset.map(lambda x: {\"labels\": x[\"Readability_Level_19\"] - 1, \"Sentence\": araby.strip_diacritics(x[\"Sentence\"])})\n",
    "blind_test_dataset = blind_test_dataset.map(lambda x: {\"Sentence\": araby.strip_diacritics(x[\"Sentence\"])})\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:53.903809Z",
     "iopub.status.busy": "2025-07-21T19:34:53.903580Z",
     "iopub.status.idle": "2025-07-21T19:34:55.844955Z",
     "shell.execute_reply": "2025-07-21T19:34:55.844353Z",
     "shell.execute_reply.started": "2025-07-21T19:34:53.903790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.utils.charmap import CharMapper\n",
    "\n",
    "arclean = CharMapper.builtin_mapper(\"arclean\")\n",
    "\n",
    "def clean_broken_arabic_words(text):\n",
    "    \"\"\" Clean broken Arabic words and remove tatweel characters.\"\"\"\n",
    "    \n",
    "    def clean_line(line, arclean):\n",
    "        return simple_word_tokenize(arclean(dediac_ar(normalize_unicode(line.strip()))))\n",
    "    \n",
    "    # Step 1: Remove broken words like الشــــــمس <- الشــــ ــمس\n",
    "    text = text.replace(' ـ', '')\n",
    "    text = text.replace('ـ ', '')\n",
    "\n",
    "    # use camel tools to clean the text, it removes tatweel characters and diacritics\n",
    "    text = \" \".join(clean_line(text, arclean))\n",
    "    return text\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = clean_broken_arabic_words(sentence)\n",
    "\n",
    "    # If sentence ends with arabic comma '،', remove it because it mess up with dependency parsing\n",
    "    if sentence.endswith('،'):\n",
    "        sentence = sentence[:-1].strip()\n",
    "\n",
    "    # Remove extra spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "        \n",
    "    # Join with space to reconstruct a clean sentence\n",
    "    return sentence\n",
    "\n",
    "def clean_example(example):\n",
    "    example[\"Sentence\"] = clean_sentence(example[\"Sentence\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:55.845804Z",
     "iopub.status.busy": "2025-07-21T19:34:55.845586Z",
     "iopub.status.idle": "2025-07-21T19:34:55.900525Z",
     "shell.execute_reply": "2025-07-21T19:34:55.899932Z",
     "shell.execute_reply.started": "2025-07-21T19:34:55.845786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clean sentence first from dataset to be able to merge \n",
    "dataset = dataset.map(clean_example)\n",
    "eval_dataset = eval_dataset.map(clean_example)\n",
    "test_dataset = test_dataset.map(clean_example)\n",
    "blind_test_dataset = blind_test_dataset.map(clean_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:55.903452Z",
     "iopub.status.busy": "2025-07-21T19:34:55.903232Z",
     "iopub.status.idle": "2025-07-21T19:34:56.111427Z",
     "shell.execute_reply": "2025-07-21T19:34:56.110828Z",
     "shell.execute_reply.started": "2025-07-21T19:34:55.903432Z"
    },
    "id": "_OAP-ONLxJPW",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Label distribution (19-level)\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x=df[\"Readability_Level_19\"])\n",
    "plt.title(\"Readability Level (19-class) Distribution\")\n",
    "plt.xlabel(\"Readability Level\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:56.112193Z",
     "iopub.status.busy": "2025-07-21T19:34:56.112014Z",
     "iopub.status.idle": "2025-07-21T19:34:56.631157Z",
     "shell.execute_reply": "2025-07-21T19:34:56.630456Z",
     "shell.execute_reply.started": "2025-07-21T19:34:56.112179Z"
    },
    "id": "MTQZoB-gxJPW",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sentence length distribution\n",
    "df[\"length\"] = df[\"Sentence\"].apply(lambda x: len(x.split()))\n",
    "sns.histplot(df[\"length\"], bins=20, kde=True)\n",
    "plt.title(\"Sentence Word Count Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:56.632068Z",
     "iopub.status.busy": "2025-07-21T19:34:56.631834Z",
     "iopub.status.idle": "2025-07-21T19:34:56.770658Z",
     "shell.execute_reply": "2025-07-21T19:34:56.769925Z",
     "shell.execute_reply.started": "2025-07-21T19:34:56.632041Z"
    },
    "id": "aCD8gh68xJPW",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Domain-wise distribution\n",
    "sns.countplot(data=df, y=\"Domain\", order=df[\"Domain\"].value_counts().index)\n",
    "plt.title(\"Domain Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:56.771686Z",
     "iopub.status.busy": "2025-07-21T19:34:56.771501Z",
     "iopub.status.idle": "2025-07-21T19:34:56.904681Z",
     "shell.execute_reply": "2025-07-21T19:34:56.903984Z",
     "shell.execute_reply.started": "2025-07-21T19:34:56.771670Z"
    },
    "id": "IWzxI8lhxJPW",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Domain-wise distribution\n",
    "sns.countplot(data=df, y=\"Domain\", order=df[\"Domain\"].value_counts().index)\n",
    "plt.title(\"Domain Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1V7NhSR2Ezu"
   },
   "source": [
    "# Feature Extraction ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:56.905562Z",
     "iopub.status.busy": "2025-07-21T19:34:56.905376Z",
     "iopub.status.idle": "2025-07-21T19:34:56.909816Z",
     "shell.execute_reply": "2025-07-21T19:34:56.909239Z",
     "shell.execute_reply.started": "2025-07-21T19:34:56.905546Z"
    },
    "id": "WIp0zfeKkqYS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def attach_morphological_features(example):\n",
    "#     sentence = example[\"Sentence\"]\n",
    "    \n",
    "#     sentence_features = morph_dict.get(sentence)\n",
    "#     for feature in morph_features:\n",
    "#         if sentence_features:\n",
    "#             feature_values = sentence_features[feature]\n",
    "#             feature_string = \" \".join(feature_values)\n",
    "#             example[feature] = feature_string\n",
    "#         else:\n",
    "#             if sentence != \"\":\n",
    "#                 print(\"cant find morph features for sentence:\", sentence)\n",
    "#             example[feature] = \"\"\n",
    "#     return example\n",
    "\n",
    "# with open(\"/kaggle/input/outputs-dep/morph_features_dict_train.json\", \"r\") as f:\n",
    "#     morph_dict_train = json.load(f)\n",
    "\n",
    "# with open(\"/kaggle/input/outputs-dep/morph_features_dict_validation.json\", \"r\") as f:\n",
    "#     morph_dict_eval = json.load(f)\n",
    "\n",
    "# with open(\"/kaggle/input/outputs-dep/morph_features_dict_test.json\", \"r\") as f:\n",
    "#     morph_dict_test = json.load(f)\n",
    "\n",
    "# # merge dicts\n",
    "# morph_dict = {**morph_dict_train, **morph_dict_eval, **morph_dict_test}\n",
    "\n",
    "# # Define the morph features resulted from dep graph you want\n",
    "# # morph_features = [\"prc3\", \"prc2\", \"prc1\", \"prc0\", \"enc0\", \"gen\", \"num\", \"cas\", \"per\", \"asp\", \"vox\", \"mod\", \"stt\", \"rat\", \"token_type\"]\n",
    "# morph_features = [\"gen\", \"num\", \"cas\", \"per\", \"asp\", \"vox\", \"mod\", \"stt\", \"rat\", \"token_type\"]\n",
    "\n",
    "# # Attach pos tags for all sentences and remove those who dont have\n",
    "# print(\"Attach morph features for dataset\")\n",
    "# dataset = dataset.map(attach_morphological_features)\n",
    "\n",
    "# print(\"Attach morph features for eval_dataset\")\n",
    "# eval_dataset = eval_dataset.map(attach_morphological_features)\n",
    "\n",
    "# print(\"Attach morph features for test_dataset\")\n",
    "# test_dataset = test_dataset.map(attach_morphological_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:56.910689Z",
     "iopub.status.busy": "2025-07-21T19:34:56.910499Z",
     "iopub.status.idle": "2025-07-21T19:34:56.928933Z",
     "shell.execute_reply": "2025-07-21T19:34:56.928268Z",
     "shell.execute_reply.started": "2025-07-21T19:34:56.910659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def build_vocab_for_sequence_feature(dataset, feature_name, add_pad_token=True):\n",
    "#     unique_tokens = set()\n",
    "#     for example in dataset:\n",
    "#         tokens = example.get(feature_name)\n",
    "#         if tokens:\n",
    "#             unique_tokens.update(tokens.split())\n",
    "#     sorted_tokens = sorted(unique_tokens)\n",
    "    \n",
    "#     vocab = {}\n",
    "#     start_idx = 0\n",
    "#     if add_pad_token:\n",
    "#         vocab['PAD'] = start_idx\n",
    "#         start_idx += 1\n",
    "    \n",
    "#     for idx, token in enumerate(sorted_tokens, start=start_idx):\n",
    "#         vocab[token] = idx\n",
    "    \n",
    "#     return vocab\n",
    "\n",
    "# def build_vocab_for_all_features(dataset, feature_names):\n",
    "#     feature_vocabs = {}\n",
    "#     for feat in feature_names:\n",
    "#         vocab = build_vocab_for_sequence_feature(dataset, feat)\n",
    "#         feature_vocabs[feat] = vocab\n",
    "#     return feature_vocabs\n",
    "\n",
    "# feature_vocabs = build_vocab_for_all_features(dataset, morph_features)\n",
    "\n",
    "# # Optional: print vocab sizes\n",
    "# for feat, vocab in feature_vocabs.items():\n",
    "#     print(f\"{feat}: {len(vocab)} unique values\")\n",
    "\n",
    "# # Save vocabs\n",
    "# import json\n",
    "# with open(\"all_feature_vocabs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(feature_vocabs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# # Load all vocabs later\n",
    "# with open(\"/kaggle/input/outputs-dep/all_feature_vocabs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     feature_vocabs = json.load(f)\n",
    "\n",
    "\n",
    "# # # Optional: print vocab sizes\n",
    "# for feat, vocab in feature_vocabs.items():\n",
    "#     print(f\"{feat}: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:56.929877Z",
     "iopub.status.busy": "2025-07-21T19:34:56.929673Z",
     "iopub.status.idle": "2025-07-21T19:34:56.947316Z",
     "shell.execute_reply": "2025-07-21T19:34:56.946607Z",
     "shell.execute_reply.started": "2025-07-21T19:34:56.929861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # try to make morph feature embedding\n",
    "# def morph_str_to_ids(feature_name, morph_string):\n",
    "#     return [feature_vocabs[feature_name][tag] for tag in morph_string.split()]\n",
    "\n",
    "# # 99th percentile length for morphemes of a sentence is: 65, so will use max_len = 64\n",
    "# def map_morph_str_to_ids(batch, feature_names=morph_features, max_len=64):\n",
    "#     for feat in feature_names:\n",
    "#         all_ids = []\n",
    "#         pad_token = \"PAD\"\n",
    "#         for morph_str in batch[feat]:\n",
    "#             tags = morph_str.split()\n",
    "#             # pad with \"PAD\" strings\n",
    "#             if len(tags) > max_len:\n",
    "#                 tags = tags[:max_len]\n",
    "#             else:\n",
    "#                 tags += [pad_token] * (max_len - len(tags))\n",
    "#             ids = morph_str_to_ids(feat, \" \".join(tags))\n",
    "#             all_ids.append(ids)\n",
    "#         batch[f\"{feat}_ids\"] = all_ids\n",
    "#     return batch\n",
    "\n",
    "# dataset = dataset.map(map_morph_str_to_ids, batched=True)\n",
    "# eval_dataset = eval_dataset.map(map_morph_str_to_ids, batched=True)\n",
    "# test_dataset = test_dataset.map(map_morph_str_to_ids, batched=True)\n",
    "\n",
    "# morph_columns = [f\"{feat}_ids\" for feat in morph_features]\n",
    "# # Set torch format for syntactic_feats to be correctly read by collator\n",
    "# dataset.set_format(\"torch\", columns=morph_columns, output_all_columns=True)\n",
    "# eval_dataset.set_format(\"torch\", columns=morph_columns, output_all_columns=True)\n",
    "# test_dataset.set_format(\"torch\", columns=morph_columns, output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:56.948421Z",
     "iopub.status.busy": "2025-07-21T19:34:56.948102Z",
     "iopub.status.idle": "2025-07-21T19:34:56.966840Z",
     "shell.execute_reply": "2025-07-21T19:34:56.966121Z",
     "shell.execute_reply.started": "2025-07-21T19:34:56.948394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# def attach_features(example, feature_dict, scaler):\n",
    "#     sentence = example[\"Sentence\"]\n",
    "#     syntactic_feats = feature_dict.get(sentence)\n",
    "\n",
    "#     if syntactic_feats is None:\n",
    "#         if sentence != \"\":\n",
    "#             print(\"Missing syntactic features:\", sentence)\n",
    "#         # replacing with mean of scaler\n",
    "#         syntactic_feats = scaler.mean_\n",
    "#     else:\n",
    "#         syntactic_feats = scaler.transform([syntactic_feats])[0]\n",
    "\n",
    "#     # example[\"syntactic_feats\"] = syntactic_feats.astype(np.float32)\n",
    "#     example[\"syntactic_feats\"] = torch.tensor(syntactic_feats, dtype=torch.float32)\n",
    "#     return example\n",
    "\n",
    "# # Load CSV\n",
    "# features_df_train = pd.read_csv(\"/kaggle/input/outputs-dep/merged_df_train.csv\")\n",
    "# features_df_eval = pd.read_csv(\"/kaggle/input/outputs-dep/merged_df_eval.csv\")\n",
    "# features_df_test = pd.read_csv(\"/kaggle/input/outputs-dep/merged_df_test.csv\")\n",
    "\n",
    "# # Load your features\n",
    "# feature_columns = [col for col in features_df_train.columns if col not in [\"Sentence\", \"labels\"]]\n",
    "# feature_dict_train = {row[\"Sentence\"]: row[feature_columns].values for _, row in features_df_train.iterrows()}\n",
    "# feature_dict_eval = {row[\"Sentence\"]: row[feature_columns].values for _, row in features_df_eval.iterrows()}\n",
    "# feature_dict_test = {row[\"Sentence\"]: row[feature_columns].values for _, row in features_df_test.iterrows()}\n",
    "\n",
    "# # Normalize only on training data\n",
    "# scaler = StandardScaler()\n",
    "# feature_matrix_train = np.stack(list(feature_dict_train.values()))\n",
    "# scaler.fit(feature_matrix_train)\n",
    "\n",
    "# # Create partials with the correct feature dicts\n",
    "# attach_features_train = partial(attach_features, feature_dict=feature_dict_train, scaler=scaler)\n",
    "# attach_features_eval = partial(attach_features, feature_dict=feature_dict_eval, scaler=scaler)\n",
    "# attach_features_test = partial(attach_features, feature_dict=feature_dict_test, scaler=scaler)\n",
    "\n",
    "# # Apply map with appropriate dict\n",
    "# print(\"Attach features for train dataset\")\n",
    "# dataset = dataset.map(attach_features_train)\n",
    "\n",
    "# print(\"Attach features for eval dataset\")\n",
    "# eval_dataset = eval_dataset.map(attach_features_eval)\n",
    "\n",
    "# print(\"Attach features for test dataset\")\n",
    "# test_dataset = test_dataset.map(attach_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:34:56.967807Z",
     "iopub.status.busy": "2025-07-21T19:34:56.967599Z",
     "iopub.status.idle": "2025-07-21T19:35:56.614477Z",
     "shell.execute_reply": "2025-07-21T19:35:56.613801Z",
     "shell.execute_reply.started": "2025-07-21T19:34:56.967792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attach dep for train dataset\n",
      "Attach dep for eval dataset\n",
      "Attach dep for test dataset\n",
      "Attach dep for blind test dataset\n"
     ]
    }
   ],
   "source": [
    "with open(\"/kaggle/input/outputs-dep/dep_graph_train.json\", \"r\") as f:\n",
    "    dep_dict_train = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/outputs-dep/dep_graph_validation.json\", \"r\") as f:\n",
    "    dep_dict_eval = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/outputs-dep/dep_graph_test.json\", \"r\") as f:\n",
    "    dep_dict_test = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/outputs-dep/dep_graph_blind_testset.json\", \"r\") as f:\n",
    "    dep_dict_blind_test = json.load(f)\n",
    "\n",
    "def attach_dep_features(example, dep_dict):\n",
    "    sentence = example[\"Sentence\"]\n",
    "    dep_graph = dep_dict.get(sentence, None)\n",
    "    if dep_graph == None:\n",
    "        print(\"cant find dep graph for sentence:\", example[\"Sentence\"])\n",
    "    example[\"dep_graph\"] = dep_graph\n",
    "    return example\n",
    "\n",
    "\n",
    "# Create partials with the correct feature dicts\n",
    "attach_dep_train = partial(attach_dep_features, dep_dict=dep_dict_train)\n",
    "attach_dep_eval = partial(attach_dep_features, dep_dict=dep_dict_eval)\n",
    "attach_dep_test = partial(attach_dep_features, dep_dict=dep_dict_test)\n",
    "attach_dep_blind_test = partial(attach_dep_features, dep_dict=dep_dict_blind_test)\n",
    "\n",
    "# Apply map with appropriate dict\n",
    "print(\"Attach dep for train dataset\")\n",
    "dataset = dataset.map(attach_dep_train)\n",
    "\n",
    "print(\"Attach dep for eval dataset\")\n",
    "eval_dataset = eval_dataset.map(attach_dep_eval)\n",
    "\n",
    "print(\"Attach dep for test dataset\")\n",
    "test_dataset = test_dataset.map(attach_dep_test)\n",
    "\n",
    "print(\"Attach dep for blind test dataset\")\n",
    "blind_test_dataset = blind_test_dataset.map(attach_dep_blind_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphNN Try ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:35:56.615579Z",
     "iopub.status.busy": "2025-07-21T19:35:56.615343Z",
     "iopub.status.idle": "2025-07-21T19:35:56.648609Z",
     "shell.execute_reply": "2025-07-21T19:35:56.647815Z",
     "shell.execute_reply.started": "2025-07-21T19:35:56.615552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_space_after_punctuation_if_not for train dataset\n",
      "add_space_after_punctuation_if_not for eval dataset\n",
      "add_space_after_punctuation_if_not for test dataset\n",
      "add_space_after_punctuation_if_not for blind test dataset\n"
     ]
    }
   ],
   "source": [
    "def add_space_after_punctuation_if_not(example):\n",
    "    sentence = example[\"Sentence\"]\n",
    "    map_english_punctuation = {\n",
    "        \";\": [\"؛\"],  # Arabic semicolon\n",
    "        \",\": [\"،\", \"٫\"],  # Arabic comma\n",
    "        \"?\": [\"؟\"],  # Arabic question mark\n",
    "        \"%\": [\"٪\"],  # Arabic percentage sign\n",
    "        \"*\": [\"۝\"],  # Arabic symbol for verse end\n",
    "    }\n",
    "    \n",
    "    # count all the punctuations found in sentence\n",
    "    punctuations_occurences = []\n",
    "    for i, letter in enumerate(sentence):\n",
    "        if letter in map_english_punctuation.keys():\n",
    "            punctuations_occurences.append(i)\n",
    "\n",
    "    # Add space after the arabic punctuation marks if its directly attached to the next word\n",
    "    offset = 0\n",
    "    for index in punctuations_occurences:\n",
    "        adjusted_index = index + offset\n",
    "        if adjusted_index < len(sentence) - 1 and sentence[adjusted_index + 1] != ' ':\n",
    "            sentence = sentence[:adjusted_index + 1] + ' ' + sentence[adjusted_index + 1:]\n",
    "            offset += 1  # Adjust offset due to inserted space\n",
    "\n",
    "    example[\"Sentence_space_after_punct\"] = sentence\n",
    "    return example\n",
    "\n",
    "# Apply map with appropriate dict\n",
    "print(\"add_space_after_punctuation_if_not for train dataset\")\n",
    "dataset = dataset.map(add_space_after_punctuation_if_not)\n",
    "\n",
    "print(\"add_space_after_punctuation_if_not for eval dataset\")\n",
    "eval_dataset = eval_dataset.map(add_space_after_punctuation_if_not)\n",
    "\n",
    "print(\"add_space_after_punctuation_if_not for test dataset\")\n",
    "test_dataset = test_dataset.map(add_space_after_punctuation_if_not)\n",
    "\n",
    "print(\"add_space_after_punctuation_if_not for blind test dataset\")\n",
    "blind_test_dataset = blind_test_dataset.map(add_space_after_punctuation_if_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:35:56.649861Z",
     "iopub.status.busy": "2025-07-21T19:35:56.649589Z",
     "iopub.status.idle": "2025-07-21T19:36:43.610386Z",
     "shell.execute_reply": "2025-07-21T19:36:43.609764Z",
     "shell.execute_reply.started": "2025-07-21T19:35:56.649836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'---': 0,\n",
       "  'IDF': 1,\n",
       "  'MOD': 2,\n",
       "  'OBJ': 3,\n",
       "  'PRD': 4,\n",
       "  'SBJ': 5,\n",
       "  'TMZ': 6,\n",
       "  'TPC': 7},\n",
       " {0: '---',\n",
       "  1: 'IDF',\n",
       "  2: 'MOD',\n",
       "  3: 'OBJ',\n",
       "  4: 'PRD',\n",
       "  5: 'SBJ',\n",
       "  6: 'TMZ',\n",
       "  7: 'TPC'},\n",
       " {'PAD': 0,\n",
       "  'ADJ': 1,\n",
       "  'ADP': 2,\n",
       "  'ADV': 3,\n",
       "  'AUX': 4,\n",
       "  'CCONJ': 5,\n",
       "  'DET': 6,\n",
       "  'INTJ': 7,\n",
       "  'NOUN': 8,\n",
       "  'NUM': 9,\n",
       "  'PART': 10,\n",
       "  'PRON': 11,\n",
       "  'PROPN': 12,\n",
       "  'PUNCT': 13,\n",
       "  'SCONJ': 14,\n",
       "  'VERB': 15,\n",
       "  'X': 16,\n",
       "  'ROOT': 17},\n",
       " {0: 'PAD',\n",
       "  1: 'ADJ',\n",
       "  2: 'ADP',\n",
       "  3: 'ADV',\n",
       "  4: 'AUX',\n",
       "  5: 'CCONJ',\n",
       "  6: 'DET',\n",
       "  7: 'INTJ',\n",
       "  8: 'NOUN',\n",
       "  9: 'NUM',\n",
       "  10: 'PART',\n",
       "  11: 'PRON',\n",
       "  12: 'PROPN',\n",
       "  13: 'PUNCT',\n",
       "  14: 'SCONJ',\n",
       "  15: 'VERB',\n",
       "  16: 'X',\n",
       "  17: 'ROOT'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab_for_dependency_relations(dataset):\n",
    "    dep_set = set()\n",
    "    for example in dataset:\n",
    "        dep_graph = example.get('dep_graph')\n",
    "        if dep_graph is None:\n",
    "            continue\n",
    "        for edge in dep_graph.get('edges', []):\n",
    "            dep = edge.get('dep')\n",
    "            if dep:\n",
    "                dep_set.add(dep)\n",
    "    return {dep: idx for idx, dep in enumerate(sorted(dep_set))}\n",
    "\n",
    "def build_vocab_for_pos_tags_in_dependency_relations(dataset):\n",
    "    pos_set = set()\n",
    "    for example in dataset:\n",
    "        dep_graph = example.get('dep_graph')\n",
    "        if dep_graph is None:\n",
    "            continue\n",
    "        for node in dep_graph.get('nodes', []):\n",
    "            pos_tags = node.get('pos_tags', [])\n",
    "            for pos in pos_tags:\n",
    "                if pos:\n",
    "                    pos_set.add(pos)\n",
    "\n",
    "    sorted_tags = sorted(pos_set)\n",
    "    vocab = {'PAD': 0}  # Make sure PAD is first\n",
    "    for idx, tag in enumerate(sorted_tags, start=1):\n",
    "        vocab[tag] = idx\n",
    "\n",
    "    vocab['ROOT'] = len(vocab)  # Add ROOT at the end\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# build dependency relations from training dataset\n",
    "dep2id = build_vocab_for_dependency_relations(dataset)\n",
    "dep_id2label = {v: k for k, v in dep2id.items()}  # invert your dep2id dict to get labels from ids\n",
    "\n",
    "# build pos tags relations from training dataset\n",
    "pos2id = build_vocab_for_pos_tags_in_dependency_relations(dataset)\n",
    "id2pos = {idx: pos for pos, idx in pos2id.items()} # invert your pos2id dict to get labels from ids\n",
    "\n",
    "dep2id, dep_id2label, pos2id, id2pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:36:43.611442Z",
     "iopub.status.busy": "2025-07-21T19:36:43.611177Z",
     "iopub.status.idle": "2025-07-21T19:36:43.621434Z",
     "shell.execute_reply": "2025-07-21T19:36:43.620801Z",
     "shell.execute_reply.started": "2025-07-21T19:36:43.611412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_graph_from_example(example):\n",
    "    # i use this Sentence_space_after_punct instead of Sentence here, in order to have same number of words in dep graph\n",
    "    # later can do it in the cleaning and make them all \"Sentence\"\n",
    "    sentence = example[\"Sentence_space_after_punct\"]\n",
    "    words = sentence.split()\n",
    "    dep_graph = example.get(\"dep_graph\")\n",
    "\n",
    "    encoding = bert_tokenizer(words, is_split_into_words=True, return_tensors='pt', add_special_tokens=True)\n",
    "    word_ids = encoding.word_ids()  # use before moving to device \n",
    "\n",
    "    # move encoding to device for faster computation\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}  # move all to device\n",
    "    input_ids = encoding['input_ids'][0]\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    embeddings = outputs.last_hidden_state[0]  # [seq_len, hidden_dim]\n",
    "\n",
    "    # word_ids = encoding.word_ids()  # len = seq_len\n",
    "    word_embeddings = []\n",
    "\n",
    "    # print(\"\\n--- Token Mapping ---\")\n",
    "    for i, word in enumerate(words):\n",
    "        indices = [j for j, wid in enumerate(word_ids) if wid == i]\n",
    "        token_pieces = [tokens[j] for j in indices]\n",
    "        if indices:\n",
    "            vecs = embeddings[indices]\n",
    "            word_embedding = vecs.mean(dim=0)\n",
    "        else:\n",
    "            word_embedding = torch.zeros(embeddings.shape[-1])\n",
    "\n",
    "        # print(f\"Word: '{word}' -> Tokens: {token_pieces}\")\n",
    "        word_embeddings.append(word_embedding)\n",
    "\n",
    "    # 👇 Add ROOT node embedding at the beginning\n",
    "    word_embeddings = [root_embedding] + word_embeddings\n",
    "\n",
    "    x = torch.stack(word_embeddings)  # [num_words, hidden_dim]\n",
    "\n",
    "    # Build edge_index and edge_attr from dep_graph\n",
    "    edge_index = [[], []]\n",
    "    edge_attr = []\n",
    "\n",
    "    if dep_graph:\n",
    "        for edge in dep_graph[\"edges\"]:\n",
    "            src = edge[\"source\"]\n",
    "            tgt = edge[\"target\"]\n",
    "            dep = edge.get(\"dep\", \"---\")\n",
    "            if src <= len(words) and tgt <= len(words):\n",
    "                edge_index[0].append(src)\n",
    "                edge_index[1].append(tgt)\n",
    "                edge_attr.append(dep2id.get(dep, 0))\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.long)\n",
    "\n",
    "    # Extract POS tag ids per node (including ROOT node)\n",
    "    pos_tag_ids_per_node = []\n",
    "\n",
    "    # Add dummy POS tags for ROOT node (if you want)\n",
    "    pos_tag_ids_per_node.append([pos2id.get(\"ROOT\", -1)])  # append root pos tag for the root node postag\n",
    "    \n",
    "    for node in dep_graph[\"nodes\"]:\n",
    "        pos_tags = node.get(\"pos_tags\", [])\n",
    "        # convert pos tags to ids, filter unknown ones\n",
    "        ids = [pos2id[pos] for pos in pos_tags if pos in pos2id]\n",
    "        if not ids:\n",
    "            # if no known pos tag, let it empty and it will be padded later\n",
    "            ids = []\n",
    "        pos_tag_ids_per_node.append(ids)\n",
    "    \n",
    "    # I want to check that we have same number of everything in the graph \n",
    "    # if x.shape[0] != edge_index.shape[1] or edge_attr.shape[0] != edge_index.shape[1]:\n",
    "    #     print(\"x.shape\", x.shape, \"edge_index.shape\", edge_index.shape, \"edge_attr.shape\", edge_attr.shape)\n",
    "    #     raise Exception(\"Shape inconsistency occured!\")    \n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, pos_tag_ids=pos_tag_ids_per_node, sentence=example[\"Sentence\"], y=torch.tensor(example[\"labels\"]), id=torch.tensor(example[\"ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:36:43.622572Z",
     "iopub.status.busy": "2025-07-21T19:36:43.622302Z",
     "iopub.status.idle": "2025-07-21T19:36:43.797447Z",
     "shell.execute_reply": "2025-07-21T19:36:43.796859Z",
     "shell.execute_reply.started": "2025-07-21T19:36:43.622546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_dep_graph(data, dep_id2label):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    sentence = data.sentence\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Add nodes with word labels\n",
    "    for i, word in enumerate(words, 1):  # nodes are 1-indexed\n",
    "        G.add_node(i, label=word)\n",
    "\n",
    "    # Add edges with dep labels\n",
    "    edge_index = data.edge_index.numpy()\n",
    "    edge_attr = data.edge_attr.numpy()\n",
    "\n",
    "    for src, tgt, dep_id in zip(edge_index[0], edge_index[1], edge_attr):\n",
    "        if src == 0:\n",
    "            src_label = \"ROOT\"\n",
    "        else:\n",
    "            src_label = G.nodes[src]['label']\n",
    "\n",
    "        tgt_label = G.nodes[tgt]['label']\n",
    "        dep_label = dep_id2label.get(dep_id, \"UNK\")\n",
    "\n",
    "        G.add_edge(src, tgt, label=dep_label)\n",
    "\n",
    "    # Plot graph\n",
    "    pos = nx.spring_layout(G)  # layout\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    nx.draw(G, pos, with_labels=True, labels=nx.get_node_attributes(G, 'label'), \n",
    "            node_size=400, node_color='lightblue', font_size=10, arrowsize=20)\n",
    "\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red', font_size=12)\n",
    "\n",
    "    plt.title(\"Dependency Graph Visualization\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:36:43.799017Z",
     "iopub.status.busy": "2025-07-21T19:36:43.798171Z",
     "iopub.status.idle": "2025-07-21T19:36:43.802158Z",
     "shell.execute_reply": "2025-07-21T19:36:43.801443Z",
     "shell.execute_reply.started": "2025-07-21T19:36:43.798989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# graph_data = build_graph_from_example(dataset[0])\n",
    "# print(\"graph_data\", graph_data)\n",
    "# visualize_dep_graph(graph_data, dep_id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:36:43.803342Z",
     "iopub.status.busy": "2025-07-21T19:36:43.803111Z",
     "iopub.status.idle": "2025-07-21T19:36:43.822969Z",
     "shell.execute_reply": "2025-07-21T19:36:43.822451Z",
     "shell.execute_reply.started": "2025-07-21T19:36:43.803327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_graphs_batch(batch):\n",
    "    batch_sentences = batch[\"Sentence_space_after_punct\"]\n",
    "    batch_words = [s.split() for s in batch_sentences]\n",
    "    batch_size = len(batch_words)\n",
    "\n",
    "    # Single tokenizer call\n",
    "    encoding = tokenizer(\n",
    "        batch_words,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    word_ids_list = [encoding.word_ids(batch_index=i) for i in range(batch_size)]\n",
    "\n",
    "    # Move to device\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "    graph_list = []\n",
    "    for i in range(batch_size):\n",
    "        words = batch_words[i]\n",
    "        sentence = batch[\"Sentence\"][i]\n",
    "        dep_graph = batch[\"dep_graph\"][i]\n",
    "        labels = batch[\"labels\"][i]\n",
    "        ids = batch[\"ID\"][i]\n",
    "\n",
    "        word_ids = word_ids_list[i]\n",
    "        input_ids = encoding[\"input_ids\"][i]\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        seq_embeddings = embeddings[i]  # [seq_len, hidden_dim]\n",
    "        word_embeddings = []\n",
    "\n",
    "        for j, word in enumerate(words):\n",
    "            indices = [k for k, wid in enumerate(word_ids) if wid == j]\n",
    "            if indices:\n",
    "                vecs = seq_embeddings[indices]\n",
    "                word_embedding = vecs.mean(dim=0)\n",
    "            else:\n",
    "                word_embedding = torch.zeros(seq_embeddings.shape[-1], device=device)\n",
    "            word_embeddings.append(word_embedding)\n",
    "\n",
    "        # Add ROOT embedding\n",
    "        word_embeddings = [root_embedding.to(device)] + word_embeddings\n",
    "        x = torch.stack(word_embeddings)\n",
    "\n",
    "        edge_index = [[], []]\n",
    "        edge_attr = []\n",
    "\n",
    "        if dep_graph:\n",
    "            for edge in dep_graph[\"edges\"]:\n",
    "                src = edge[\"source\"]\n",
    "                tgt = edge[\"target\"]\n",
    "                dep = edge.get(\"dep\", \"---\")\n",
    "                if src <= len(words) and tgt <= len(words):\n",
    "                    edge_index[0].append(src)\n",
    "                    edge_index[1].append(tgt)\n",
    "                    edge_attr.append(dep2id.get(dep, 0))\n",
    "\n",
    "        # Extract POS tag ids per node (including ROOT node)\n",
    "        pos_tag_ids_per_node = []\n",
    "        \n",
    "        # Add dummy POS tags for ROOT node (if you want)\n",
    "        pos_tag_ids_per_node.append([pos2id.get(\"ROOT\", -1)])  # append -1 for the root node postag\n",
    "        \n",
    "        for node in dep_graph[\"nodes\"]:\n",
    "            pos_tags = node.get(\"pos_tags\", [])\n",
    "            # convert pos tags to ids, filter unknown ones\n",
    "            ids = [pos2id[pos] for pos in pos_tags if pos in pos2id]\n",
    "            if not ids:\n",
    "                # if no known pos tag, let it empty and it will be padded later\n",
    "                ids = []\n",
    "            pos_tag_ids_per_node.append(ids)\n",
    "        \n",
    "        # Pad variable length lists to fixed length\n",
    "        max_len = max(len(ids) for ids in pos_tag_ids_per_node)\n",
    "        padded_pos_tag_ids = [\n",
    "            ids + [pos2id.get(\"PAD\", -1)] * (max_len - len(ids)) for ids in pos_tag_ids_per_node\n",
    "        ]\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long, device=device)\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.long, device=device)\n",
    "        pos_tag_ids_per_node = torch.tensor(padded_pos_tag_ids, dtype=torch.long, device=device)  \n",
    "        y = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        id = torch.tensor(ids, dtype=torch.long, device=device)\n",
    "\n",
    "        graph = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            pos_tag_ids=pos_tag_ids_per_node,\n",
    "            sentence=sentence,\n",
    "            y=y,\n",
    "            id=id,\n",
    "        )\n",
    "        graph_list.append(graph)\n",
    "\n",
    "    return graph_list\n",
    "\n",
    "def build_graphs_batch_v2(batch):\n",
    "    batch_sentences = batch[\"Sentence_space_after_punct\"]\n",
    "    batch_words = [s.split() for s in batch_sentences]\n",
    "    batch_size = len(batch_words)\n",
    "\n",
    "    graph_list = []\n",
    "    for i in range(batch_size):\n",
    "        words = batch_words[i]\n",
    "        sentence = batch[\"Sentence\"][i]\n",
    "        dep_graph = batch[\"dep_graph\"][i]\n",
    "        labels = batch[\"labels\"][i] if \"labels\" in batch else None\n",
    "        sample_id = batch[\"ID\"][i]\n",
    "\n",
    "        edge_index = [[], []]\n",
    "        edge_attr = []\n",
    "\n",
    "        if dep_graph:\n",
    "            for edge in dep_graph[\"edges\"]:\n",
    "                src = edge[\"source\"]\n",
    "                tgt = edge[\"target\"]\n",
    "                dep = edge.get(\"dep\", \"---\")\n",
    "                if src <= len(words) and tgt <= len(words):\n",
    "                    edge_index[0].append(src)\n",
    "                    edge_index[1].append(tgt)\n",
    "                    edge_attr.append(dep2id.get(dep, 0))\n",
    "\n",
    "        # Extract POS tag ids per node (including ROOT node)\n",
    "        pos_tag_ids_per_node = []\n",
    "        pos_tag_ids_per_node.append([pos2id.get(\"ROOT\", -1)])  # Root POS\n",
    "\n",
    "        for node in dep_graph[\"nodes\"]:\n",
    "            pos_tags = node.get(\"pos_tags\", [])\n",
    "            ids = [pos2id[pos] for pos in pos_tags if pos in pos2id]\n",
    "            pos_tag_ids_per_node.append(ids if ids else [])\n",
    "\n",
    "        max_len = max(len(ids) for ids in pos_tag_ids_per_node)\n",
    "        padded_pos_tag_ids = [\n",
    "            ids + [pos2id.get(\"PAD\", -1)] * (max_len - len(ids)) for ids in pos_tag_ids_per_node\n",
    "        ]\n",
    "        \n",
    "        graph = Data(\n",
    "            # Do NOT include x here; model will compute it\n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.long),\n",
    "            pos_tag_ids=torch.tensor(padded_pos_tag_ids, dtype=torch.long),\n",
    "            sentence=sentence,\n",
    "            sentence_tokenized=words,  # pass to model for BERT\n",
    "            # y=torch.tensor(labels, dtype=torch.long),\n",
    "            id=torch.tensor(sample_id, dtype=torch.long),\n",
    "            num_nodes = len(words) + 1,  # +1 for the ROOT node\n",
    "        )\n",
    "\n",
    "        # to make sure it works for blind testset too\n",
    "        if labels is not None:\n",
    "            graph.y = torch.tensor(labels, dtype=torch.long)\n",
    "        else:\n",
    "            # place holder for official blindtestset, 0 : because it must belong to 0, numclasses - 1\n",
    "            graph.y = torch.tensor([0], dtype=torch.long)\n",
    "\n",
    "\n",
    "        graph_list.append(graph)\n",
    "\n",
    "    return graph_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:36:43.823871Z",
     "iopub.status.busy": "2025-07-21T19:36:43.823678Z",
     "iopub.status.idle": "2025-07-21T19:37:34.960273Z",
     "shell.execute_reply": "2025-07-21T19:37:34.959618Z",
     "shell.execute_reply.started": "2025-07-21T19:36:43.823856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_graph_from_example for training dataset\n",
      "build_graph_from_example for eval dataset\n",
      "build_graph_from_example for test dataset\n",
      "build_graph_from_example for blind test dataset\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"build_graph_from_example for training dataset\")\n",
    "graph_list_train = []\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    batch = dataset[i:i+batch_size]\n",
    "    graph_list_train.extend(build_graphs_batch_v2(batch))\n",
    "torch.save(graph_list_train, \"graph_list_train.pt\")\n",
    "\n",
    "print(\"build_graph_from_example for eval dataset\")\n",
    "graph_list_eval = []\n",
    "for i in range(0, len(eval_dataset), batch_size):\n",
    "    batch = eval_dataset[i:i+batch_size]\n",
    "    graph_list_eval.extend(build_graphs_batch_v2(batch))\n",
    "torch.save(graph_list_eval, \"graph_list_eval.pt\")\n",
    "\n",
    "print(\"build_graph_from_example for test dataset\")\n",
    "graph_list_test = []\n",
    "for i in range(0, len(test_dataset), batch_size):\n",
    "    batch = test_dataset[i:i+batch_size]\n",
    "    graph_list_test.extend(build_graphs_batch_v2(batch))\n",
    "torch.save(graph_list_test, \"graph_list_test.pt\")\n",
    "\n",
    "print(\"build_graph_from_example for blind test dataset\")\n",
    "graph_list_blind_test = []\n",
    "for i in range(0, len(blind_test_dataset), batch_size):\n",
    "    batch = blind_test_dataset[i:i+batch_size]\n",
    "    graph_list_blind_test.extend(build_graphs_batch_v2(batch))\n",
    "torch.save(graph_list_test, \"graph_list_blind_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T19:37:34.964450Z",
     "iopub.status.busy": "2025-07-21T19:37:34.964247Z",
     "iopub.status.idle": "2025-07-21T19:37:34.968075Z",
     "shell.execute_reply": "2025-07-21T19:37:34.967408Z",
     "shell.execute_reply.started": "2025-07-21T19:37:34.964434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LOADING them if already existed\n",
    "\n",
    "# import torch\n",
    "# from torch_geometric.data.data import DataEdgeAttr\n",
    "\n",
    "# # Allowlist the PyG class you need\n",
    "# torch.serialization.add_safe_globals([DataEdgeAttr])\n",
    "\n",
    "# # Then load your data with weights_only=False\n",
    "# graph_list_train = torch.load(\"graph_list_train.pt\", weights_only=False)\n",
    "# graph_list_eval = torch.load(\"graph_list_eval.pt\", weights_only=False)\n",
    "# graph_list_test = torch.load(\"graph_list_test.pt\", weights_only=False)\n",
    "# graph_list_blind_test = torch.load(\"graph_list_blind_test.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:37:07.199541Z",
     "iopub.status.busy": "2025-07-21T23:37:07.199226Z",
     "iopub.status.idle": "2025-07-21T23:37:07.238379Z",
     "shell.execute_reply": "2025-07-21T23:37:07.237692Z",
     "shell.execute_reply.started": "2025-07-21T23:37:07.199516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch_geometric.nn import NNConv, GCNConv, global_mean_pool, GATv2Conv, NNConv, TransformerConv\n",
    "from torch_geometric.nn.aggr import AttentionalAggregation\n",
    "from torch_geometric.nn import TransformerConv, GATConv, GraphNorm\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from coral_pytorch.losses import coral_loss\n",
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "\n",
    "class WeightedKappaLoss(nn.Module):\n",
    "    # Implementing paper: Weighted kappa loss function for multi-class classification of ordinal data in deep learning\n",
    "    def __init__(self, num_classes, mode='quadratic', eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        assert mode in ['linear', 'quadratic'], \"Mode must be 'linear' or 'quadratic'\"\n",
    "        self.mode = mode\n",
    "        self.eps = eps\n",
    "\n",
    "        # Create weight matrix omega\n",
    "        labels = torch.arange(num_classes, dtype=torch.float32)\n",
    "        diff = labels.unsqueeze(0) - labels.unsqueeze(1)  # shape (C,C)\n",
    "        if mode == 'linear':\n",
    "            weights = diff.abs()\n",
    "        else:\n",
    "            weights = diff.pow(2)\n",
    "\n",
    "        # Normalize weights by max to keep in [0,1]\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # Register as buffer for device compatibility\n",
    "        self.register_buffer('weights', weights)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        y_pred: Float tensor of shape (N, C), probabilities (softmax outputs)\n",
    "        y_true: Long tensor of shape (N,), true class indices\n",
    "        \"\"\"\n",
    "\n",
    "        device = y_pred.device\n",
    "        N = y_pred.size(0)\n",
    "        C = self.num_classes\n",
    "\n",
    "        # One-hot encode y_true: shape (N, C)\n",
    "        y_true_onehot = torch.zeros((N, C), device=device)\n",
    "        y_true_onehot.scatter_(1, y_true.unsqueeze(1), 1)\n",
    "\n",
    "        # Normalize y_pred probabilities to sum to 1 (if not already)\n",
    "        y_pred = y_pred / (y_pred.sum(dim=1, keepdim=True) + self.eps)\n",
    "\n",
    "        # Observed matrix O_{i,j} = sum over samples of predicted prob class i and true class j\n",
    "        O = y_pred.T @ y_true_onehot  # shape (C, C)\n",
    "\n",
    "        # Marginals for predictions and truth\n",
    "        hist_pred = y_pred.sum(dim=0)  # shape (C,)\n",
    "        hist_true = y_true_onehot.sum(dim=0)  # shape (C,)\n",
    "\n",
    "        # Expected matrix E_{i,j} = outer product of marginals normalized by number of samples\n",
    "        E = torch.outer(hist_pred, hist_true) / N  # shape (C, C)\n",
    "\n",
    "        # Weighted sums\n",
    "        weighted_O = (self.weights * O).sum()\n",
    "        weighted_E = (self.weights * E).sum()\n",
    "\n",
    "        # Weighted kappa\n",
    "        kappa = 1 - (weighted_O / (weighted_E + self.eps))\n",
    "\n",
    "        # Loss is log(1 - kappa) as per paper\n",
    "        # from paper: \n",
    "        # However, optimization of the loss function (L ) is normally presented as a minimization problem, therefore in Eq. (4) we present\n",
    "        # the same problem converted to minimization. Notice that in this case, we propose to take the logarithm of the index, in order to increase the penalization of incorrect assignments.\n",
    "        # minimize L = log (1 − κ ) where L ∈ (−∞, log 2] (4)\n",
    "        loss = torch.log(1 - kappa + self.eps)\n",
    "        return loss\n",
    "\n",
    "class ImprovedSimpleEdgeGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_relations, num_pos_tags, pos_emb_dim=63, dropout=0.1, conv_layers=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv_layers = conv_layers\n",
    "        self.wkl = WeightedKappaLoss(num_classes)\n",
    "        self.pos_emb = nn.Embedding(num_pos_tags, pos_emb_dim)\n",
    "\n",
    "        edge_embedding_size = hidden_dim\n",
    "        self.edge_emb = nn.Embedding(num_relations, edge_embedding_size)\n",
    "        # Try larger too ?\n",
    "        self.edge_proj = nn.Sequential(\n",
    "            nn.Linear(edge_embedding_size, edge_embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(edge_embedding_size, edge_embedding_size)\n",
    "        )\n",
    "\n",
    "        input_dim_total = input_dim + pos_emb_dim\n",
    "        self.node_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim_total, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.edge_fuse_proj = nn.Linear(2 * edge_embedding_size, hidden_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.conv_layers = conv_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        num_heads = 4\n",
    "        for _ in range(conv_layers):\n",
    "            self.convs.append(TransformerConv(in_channels=hidden_dim, out_channels=hidden_dim, heads=num_heads, concat=False, edge_dim=edge_embedding_size))  # replace with your conv class\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        self.att_pool = AttentionalAggregation(\n",
    "            gate_nn=nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes - 1)\n",
    "        self.fc_wkl = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def aggregate_edge_features(self, x, edge_index, edge_features):\n",
    "        src_nodes = edge_index[0]\n",
    "        dst_nodes = edge_index[1]\n",
    "    \n",
    "        # Init aggregation tensors\n",
    "        src_counts = torch.zeros(x.size(0), device=x.device)\n",
    "        agg_edge_feats_src = torch.zeros(x.size(0), edge_features.size(1), device=x.device)\n",
    "        src_counts.index_add_(0, src_nodes, torch.ones_like(src_nodes, dtype=torch.float))\n",
    "        agg_edge_feats_src.index_add_(0, src_nodes, edge_features)\n",
    "    \n",
    "        dst_counts = torch.zeros(x.size(0), device=x.device)\n",
    "        agg_edge_feats_dst = torch.zeros(x.size(0), edge_features.size(1), device=x.device)\n",
    "        dst_counts.index_add_(0, dst_nodes, torch.ones_like(dst_nodes, dtype=torch.float))\n",
    "        agg_edge_feats_dst.index_add_(0, dst_nodes, edge_features)\n",
    "    \n",
    "        # Normalize\n",
    "        src_counts = src_counts.clamp(min=1).unsqueeze(-1)\n",
    "        dst_counts = dst_counts.clamp(min=1).unsqueeze(-1)\n",
    "        agg_edge_feats_src = agg_edge_feats_src / src_counts\n",
    "        agg_edge_feats_dst = agg_edge_feats_dst / dst_counts\n",
    "    \n",
    "        # Fuse\n",
    "        agg_edge_feats = torch.cat([agg_edge_feats_src, agg_edge_feats_dst], dim=-1)\n",
    "        agg_edge_feats = self.edge_fuse_proj(agg_edge_feats)\n",
    "        return agg_edge_feats\n",
    "    \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, y = data.x, data.y\n",
    "        edge_index, edge_attr, batch = data.edge_index, data.edge_attr, data.batch\n",
    "        pos_tag_ids = data.pos_tag_ids  # [N, 4]\n",
    "        mask = (pos_tag_ids != 0).unsqueeze(-1)  # [N, 4, 1]\n",
    "        \n",
    "        pos_emb = self.pos_emb(pos_tag_ids)  # [N, 4, 32]\n",
    "        pos_emb = pos_emb * mask.float()     # zero out padding embeddings\n",
    "        \n",
    "        # Compute masked average\n",
    "        sum_emb = pos_emb.sum(dim=1)  # [N, 32]\n",
    "        valid_counts = mask.sum(dim=1).clamp(min=1)  # avoid division by zero → [N, 1]\n",
    "        pos_features = sum_emb / valid_counts  # [N, 32]\n",
    "\n",
    "        edge_features = self.edge_emb(edge_attr.squeeze())  # [num_edges, edge_dim]\n",
    "        edge_features = self.edge_proj(edge_features)      # project to hidden_dim back\n",
    "        # edge_features = F.relu(self.edge_proj(edge_features))  # project to hidden_dim back and relu\n",
    "        \n",
    "        x = torch.cat([x, pos_features], dim=-1) # add pos to x\n",
    "        x = self.node_proj(x)\n",
    "        x = F.relu(x)\n",
    "        x_res = x # save x to use it in the very end \n",
    "        \n",
    "        x_prev = x\n",
    "        for i in range(self.conv_layers):\n",
    "            x_new = self.convs[i](x_prev, edge_index, edge_features)\n",
    "            x_new = self.norms[i](x_new)\n",
    "            x_new = F.relu(x_new)\n",
    "            x_new = self.dropout(x_new)\n",
    "            \n",
    "            # Residual from previous layer and edge aggregation\n",
    "            x = x_prev + x_new\n",
    "            x = x + self.aggregate_edge_features(x, edge_index, edge_features)\n",
    "            \n",
    "            x_prev = x  # update for next iteration\n",
    "            \n",
    "        x = x + x_res  # original input residual\n",
    "\n",
    "                \n",
    "        x_pool = self.att_pool(x, batch)\n",
    "\n",
    "        # Improves stability before the final classifier. This layer doesn’t add trainable weights — it just normalizes.\n",
    "        x_pool = F.layer_norm(x_pool, x_pool.size()[1:])\n",
    "        logits = self.fc(x_pool)\n",
    "        \n",
    "        # coral loss\n",
    "        levels = levels_from_labelbatch(y, self.num_classes).to(logits.device)\n",
    "        loss = coral_loss(logits, levels)\n",
    "\n",
    "        # wkl loss\n",
    "        logits_wkl = self.fc_wkl(x_pool)\n",
    "        probabilities = torch.softmax(logits_wkl, dim=1)\n",
    "        wkl_loss = self.wkl(probabilities, y)\n",
    "        \n",
    "        loss = 0.5 * loss + 0.5 * wkl_loss\n",
    "\n",
    "        # predict from coral head, since it provided better results in general\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "class StrongerEdgeGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_relations, num_pos_tags,\n",
    "                 pos_emb_dim=63, dropout=0.1, conv_layers=4, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv_layers = conv_layers\n",
    "        self.wkl = WeightedKappaLoss(num_classes)\n",
    "        self.pos_emb = nn.Embedding(num_pos_tags, pos_emb_dim)\n",
    "\n",
    "        self.edge_emb = nn.Embedding(num_relations, hidden_dim)\n",
    "        self.edge_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        input_dim_total = input_dim + pos_emb_dim\n",
    "        self.node_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim_total, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Edge gating mechanism\n",
    "        self.edge_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.edge_fuse_proj = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        for i in range(conv_layers):\n",
    "            if i % 2 == 0:\n",
    "                self.convs.append(TransformerConv(hidden_dim, hidden_dim, heads=num_heads, concat=False, edge_dim=hidden_dim))\n",
    "            else:\n",
    "                self.convs.append(GATv2Conv(hidden_dim, hidden_dim, heads=num_heads, concat=False, edge_dim=hidden_dim))    \n",
    "            self.norms.append(GraphNorm(hidden_dim))\n",
    "\n",
    "        # Multi-head attention pooling over graph nodes\n",
    "        self.global_query = nn.Parameter(torch.randn(1, hidden_dim))\n",
    "        self.att_pool = MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes - 1)\n",
    "        self.fc_wkl = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def aggregate_edge_features(self, x, edge_index, edge_features):\n",
    "        src_nodes = edge_index[0]\n",
    "        dst_nodes = edge_index[1]\n",
    "        x_src = x[src_nodes]\n",
    "        x_dst = x[dst_nodes]\n",
    "\n",
    "        # Gating\n",
    "        gate_input = torch.cat([x_src, x_dst, edge_features], dim=-1)\n",
    "        gates = self.edge_gate(gate_input)\n",
    "        gated_edge_feats = gates * edge_features\n",
    "\n",
    "        # Aggregate\n",
    "        agg_edge_feats_src = torch.zeros(x.size(0), edge_features.size(1), device=x.device)\n",
    "        agg_edge_feats_src.index_add_(0, src_nodes, gated_edge_feats)\n",
    "\n",
    "        agg_edge_feats_dst = torch.zeros(x.size(0), edge_features.size(1), device=x.device)\n",
    "        agg_edge_feats_dst.index_add_(0, dst_nodes, gated_edge_feats)\n",
    "\n",
    "        agg = (agg_edge_feats_src + agg_edge_feats_dst) / 2\n",
    "        return self.edge_fuse_proj(agg)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, y = data.x, data.y\n",
    "        edge_index, edge_attr, batch = data.edge_index, data.edge_attr, data.batch\n",
    "        pos_tag_ids = data.pos_tag_ids\n",
    "\n",
    "        mask = (pos_tag_ids != 0).unsqueeze(-1)\n",
    "        pos_emb = self.pos_emb(pos_tag_ids) * mask.float()\n",
    "        pos_features = pos_emb.sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        edge_features = self.edge_proj(self.edge_emb(edge_attr.squeeze()))\n",
    "\n",
    "        x = torch.cat([x, pos_features], dim=-1)\n",
    "        x = F.relu(self.node_proj(x))\n",
    "        x_res = x\n",
    "\n",
    "        x_prev = x\n",
    "        for i in range(self.conv_layers):\n",
    "            if isinstance(self.convs[i], TransformerConv):\n",
    "                x_new = self.convs[i](x_prev, edge_index, edge_features)\n",
    "            else:\n",
    "                x_new = self.convs[i](x_prev, edge_index)\n",
    "\n",
    "            x_new = self.norms[i](x_new, batch)\n",
    "            x_new = F.relu(x_new)\n",
    "            x_new = self.dropout(x_new)\n",
    "\n",
    "            x = x_prev + x_new + self.aggregate_edge_features(x_new, edge_index, edge_features)\n",
    "            x_prev = x\n",
    "\n",
    "        x = x + x_res\n",
    "\n",
    "        # Attention pooling: batch graph → [batch_size, num_nodes, hidden_dim]        \n",
    "        # Gather node embeddings per graph\n",
    "        graphs = []\n",
    "        for i in range(batch.max().item() + 1):\n",
    "            node_indices = (batch == i).nonzero(as_tuple=True)[0]\n",
    "            graphs.append(x[node_indices])\n",
    "        \n",
    "        # Pad graphs to same length\n",
    "        padded = pad_sequence(graphs, batch_first=True)   # [B, N_max, H]\n",
    "        mask = torch.ones(padded.size()[:2], dtype=torch.bool, device=x.device)\n",
    "        for i, g in enumerate(graphs):\n",
    "            mask[i, len(g):] = False\n",
    "        \n",
    "        # Attention pooling\n",
    "        query = self.global_query.expand(padded.size(0), 1, -1)  # [B, 1, H]\n",
    "        pooled, _ = self.att_pool(query, padded, padded, key_padding_mask=~mask)\n",
    "        x_pool = pooled.squeeze(1)\n",
    "\n",
    "        x_pool = F.layer_norm(x_pool, x_pool.size()[1:])\n",
    "        logits = self.fc(x_pool)\n",
    "\n",
    "        levels = levels_from_labelbatch(y, self.num_classes).to(logits.device)\n",
    "        loss = coral_loss(logits, levels)\n",
    "\n",
    "        logits_wkl = self.fc_wkl(x_pool)\n",
    "        probabilities = torch.softmax(logits_wkl, dim=1)\n",
    "        wkl_loss = self.wkl(probabilities, y)\n",
    "\n",
    "        loss = 0.2 * loss + 0.8 * wkl_loss\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "class BERTGraphModel(nn.Module):\n",
    "    def __init__(self, gnn_model, tokenizer, bert_model):\n",
    "        super().__init__()\n",
    "        self.gnn_model = gnn_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # batch is a PyG Batch (collection of Data objects with sentence, etc.)\n",
    "        batch_size = batch.num_graphs\n",
    "        sentences = batch.sentence  # List[str], len == batch_size\n",
    "        batch_words = batch.sentence_tokenized\n",
    "\n",
    "        # Step 1: Tokenize sentences\n",
    "        encoding = self.tokenizer(\n",
    "            batch_words,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        word_ids_list = [encoding.word_ids(batch_index=i) for i in range(batch_size)]\n",
    "\n",
    "        # Move to device\n",
    "        encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        # outputs = self.bert_model(**encoding)\n",
    "        # hidden_states = outputs.last_hidden_state  # [B, seq_len, hidden]\n",
    "        \n",
    "        outputs = self.bert_model(**encoding, output_hidden_states=True)\n",
    "        hidden_states_all = outputs.hidden_states  # tuple of (13,) [embedding + 12 hidden layers]\n",
    "        \n",
    "        # Example: sum last 4 layers (you can also learn weights)\n",
    "        last_4_layers = hidden_states_all[-4:]  # list of tensors [B, seq_len, hidden_dim]\n",
    "        hidden_states = torch.stack(last_4_layers).mean(dim=0)  # average last 4 layers\n",
    "        \n",
    "        # Step 2: Convert BERT embeddings to word-level x features\n",
    "        x_list = []\n",
    "        for i in range(batch_size):\n",
    "            words = batch_words[i]\n",
    "            word_ids = word_ids_list[i]\n",
    "            token_embeds = hidden_states[i]\n",
    "            num_words = len(batch.pos_tag_ids[i]) - 1  # exclude ROOT\n",
    "\n",
    "            word_embeddings = []\n",
    "            for j, word in enumerate(words):\n",
    "                indices = [k for k, wid in enumerate(word_ids) if wid == j]\n",
    "                if indices:\n",
    "                    vecs = token_embeds[indices]\n",
    "                    word_emb = vecs.mean(dim=0)\n",
    "                else:\n",
    "                    word_emb = torch.zeros(token_embeds.size(-1), device=token_embeds.device)\n",
    "                word_embeddings.append(word_emb)\n",
    "            \n",
    "            # Use the [CLS] token embedding as root node for the sentence\n",
    "            cls_emb = token_embeds[0]  # [CLS] is always at position 0 in BERT output\n",
    "            word_embeddings = [cls_emb] + word_embeddings\n",
    "\n",
    "            x_graph = torch.stack(word_embeddings, dim=0)  # [N_nodes, input_dim]\n",
    "            x_list.append(x_graph)\n",
    "\n",
    "        # Step 3: Concatenate and inject into the PyG Batch\n",
    "        x_full = torch.cat(x_list, dim=0)  # [total_nodes_across_batch, input_dim]\n",
    "        batch.x = x_full\n",
    "\n",
    "        # Step 4: Pass to GNN\n",
    "        return self.gnn_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:37:07.277834Z",
     "iopub.status.busy": "2025-07-21T23:37:07.277344Z",
     "iopub.status.idle": "2025-07-21T23:37:07.589314Z",
     "shell.execute_reply": "2025-07-21T23:37:07.588662Z",
     "shell.execute_reply.started": "2025-07-21T23:37:07.277813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad_pos_tags(graph_list):\n",
    "    max_len = max(data.pos_tag_ids.size(1) for data in graph_list)\n",
    "    pad_id = pos2id.get(\"PAD\", -1)\n",
    "    for data in graph_list:\n",
    "        cur_len = data.pos_tag_ids.size(1)\n",
    "        if cur_len < max_len:\n",
    "            padding = torch.full(\n",
    "                (data.pos_tag_ids.size(0), max_len - cur_len),\n",
    "                pad_id,\n",
    "                dtype=data.pos_tag_ids.dtype,\n",
    "                device=data.pos_tag_ids.device\n",
    "            )\n",
    "            data.pos_tag_ids = torch.cat([data.pos_tag_ids, padding], dim=1)\n",
    "    return graph_list\n",
    "\n",
    "graph_list_train = pad_pos_tags(graph_list_train)\n",
    "graph_list_eval = pad_pos_tags(graph_list_eval)\n",
    "graph_list_test = pad_pos_tags(graph_list_test)\n",
    "graph_list_blind_test = pad_pos_tags(graph_list_blind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:37:07.590552Z",
     "iopub.status.busy": "2025-07-21T23:37:07.590333Z",
     "iopub.status.idle": "2025-07-21T23:37:07.705697Z",
     "shell.execute_reply": "2025-07-21T23:37:07.705065Z",
     "shell.execute_reply.started": "2025-07-21T23:37:07.590533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for data in graph_list_train:\n",
    "    if hasattr(data, 'edge_attr') and data.edge_attr.dim() == 1:\n",
    "        data.edge_attr = data.edge_attr.unsqueeze(1)  # add feature dimension\n",
    "\n",
    "for data in graph_list_eval:\n",
    "    if hasattr(data, 'edge_attr') and data.edge_attr.dim() == 1:\n",
    "        data.edge_attr = data.edge_attr.unsqueeze(1)  # add feature dimension\n",
    "\n",
    "for data in graph_list_test:\n",
    "    if hasattr(data, 'edge_attr') and data.edge_attr.dim() == 1:\n",
    "        data.edge_attr = data.edge_attr.unsqueeze(1)  # add feature dimension\n",
    "\n",
    "for data in graph_list_blind_test:\n",
    "    if hasattr(data, 'edge_attr') and data.edge_attr.dim() == 1:\n",
    "        data.edge_attr = data.edge_attr.unsqueeze(1)  # add feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:37:07.841436Z",
     "iopub.status.busy": "2025-07-21T23:37:07.841196Z",
     "iopub.status.idle": "2025-07-21T23:37:07.848707Z",
     "shell.execute_reply": "2025-07-21T23:37:07.848083Z",
     "shell.execute_reply.started": "2025-07-21T23:37:07.841408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "# train_loader = DataLoader(graph_list_train, batch_size=batch_size, shuffle=True)\n",
    "# val_loader   = DataLoader(graph_list_eval, batch_size=batch_size)\n",
    "# test_loader  = DataLoader(graph_list_test, batch_size=batch_size)\n",
    "\n",
    "# To make it faster and experiment better !!\n",
    "# train_loader = DataLoader(graph_list_train[:int(len(graph_list_train))], batch_size=batch_size, shuffle=True)\n",
    "# val_loader   = DataLoader(graph_list_eval[:int(len(graph_list_eval))], batch_size=batch_size)\n",
    "test_loader  = DataLoader(graph_list_test[:int(len(graph_list_test))], batch_size=batch_size)\n",
    "blind_test_loader  = DataLoader(graph_list_blind_test[:int(len(graph_list_blind_test))], batch_size=batch_size)\n",
    "\n",
    "# now we will train on train,val\n",
    "# validate on internal test\n",
    "# final testing on the official blind test\n",
    "combined_train_val = graph_list_train + graph_list_eval  # combine lists\n",
    "combined_train_loader = DataLoader(combined_train_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:37:57.195930Z",
     "iopub.status.busy": "2025-07-21T23:37:57.195147Z",
     "iopub.status.idle": "2025-07-21T23:37:58.816394Z",
     "shell.execute_reply": "2025-07-21T23:37:58.815586Z",
     "shell.execute_reply.started": "2025-07-21T23:37:57.195904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, cohen_kappa_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 19  # since 1-19\n",
    "bert_model_ckpt = \"aubmindlab/bert-base-arabertv02\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_ckpt)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_ckpt).to(device)\n",
    "\n",
    "# try 1024, with 6 layers of conv and bert 8 layers frozen ? \n",
    "# gnn_model = ImprovedSimpleEdgeGNN(input_dim=768, hidden_dim=512, num_classes=NUM_CLASSES, num_relations=len(dep2id), num_pos_tags=len(pos2id), conv_layers=16).to(device)\n",
    "gnn_model = StrongerEdgeGNN(input_dim=768, hidden_dim=512, num_classes=NUM_CLASSES, num_relations=len(dep2id), num_pos_tags=len(pos2id), pos_emb_dim=32, dropout=0.1, conv_layers=16, num_heads=4).to(device)\n",
    "model = BERTGraphModel(gnn_model, bert_tokenizer, bert_model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def coral_decode(logits):\n",
    "    \"\"\"\n",
    "    logits: [batch_size, num_classes-1] tensor\n",
    "    returns: numpy array of predicted class labels\n",
    "    \"\"\"\n",
    "    probas = torch.sigmoid(logits)  # sigmoid over logits\n",
    "    # For each sample, count how many thresholds are > 0.5\n",
    "    preds = (probas > 0.5).sum(dim=1)\n",
    "    return preds\n",
    "    \n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            logits = out[\"logits\"]\n",
    "\n",
    "            # for coral\n",
    "            preds = coral_decode(logits)\n",
    "\n",
    "            # CE decode\n",
    "            # preds = logits.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "    return {\"accuracy\": acc, \"qwk\": qwk}\n",
    "\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            # CE decode\n",
    "            # preds = logits.argmax(dim=1)\n",
    "            \n",
    "            # coral decode\n",
    "            preds = coral_decode(logits)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:37:58.817984Z",
     "iopub.status.busy": "2025-07-21T23:37:58.817681Z",
     "iopub.status.idle": "2025-07-21T23:37:58.822996Z",
     "shell.execute_reply": "2025-07-21T23:37:58.822278Z",
     "shell.execute_reply.started": "2025-07-21T23:37:58.817951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Freeze only the lower layers (first 8 of 12):\n",
    "for name, param in bert_model.named_parameters():\n",
    "    if name.startswith(\"encoder.layer.\") and int(name.split(\".\")[2]) < 8:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:37:58.824444Z",
     "iopub.status.busy": "2025-07-21T23:37:58.823816Z",
     "iopub.status.idle": "2025-07-21T23:37:58.841893Z",
     "shell.execute_reply": "2025-07-21T23:37:58.841180Z",
     "shell.execute_reply.started": "2025-07-21T23:37:58.824419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 143023973\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")\n",
    "\n",
    "# to check if bert is also trainable \n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-21T23:40:20.056Z",
     "iopub.execute_input": "2025-07-21T23:38:00.937882Z",
     "iopub.status.busy": "2025-07-21T23:38:00.937346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_acc = 0.0\n",
    "best_qwk = 0.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in combined_train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "\n",
    "        # For CORAL, loss is returned from model\n",
    "        loss = out[\"loss\"]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    val_acc = evaluate(model, test_loader)\n",
    "    val_accuracy = val_acc['accuracy']\n",
    "    val_qwk = val_acc['qwk']\n",
    "\n",
    "    # Save best accuracy model\n",
    "    if val_accuracy > best_acc:\n",
    "        best_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'model_best_acc.pt')\n",
    "        print(f\"✅ Saved new best accuracy model: {best_acc:.4f}\")\n",
    "\n",
    "    # Save best QWK model\n",
    "    if val_qwk > best_qwk:\n",
    "        best_qwk = val_qwk\n",
    "        torch.save(model.state_dict(), 'model_best_qwk.pt')\n",
    "        print(f\"✅ Saved new best QWK model: {best_qwk:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}, Loss: {total_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}, Val QWK: {val_qwk:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'last_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:36:43.098128Z",
     "iopub.status.busy": "2025-07-21T23:36:43.097545Z",
     "iopub.status.idle": "2025-07-21T23:36:57.576681Z",
     "shell.execute_reply": "2025-07-21T23:36:57.575888Z",
     "shell.execute_reply.started": "2025-07-21T23:36:43.098103Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6,\n",
       "  3,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  10,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  14,\n",
       "  7,\n",
       "  3,\n",
       "  9,\n",
       "  12,\n",
       "  10,\n",
       "  12,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  12,\n",
       "  6,\n",
       "  9,\n",
       "  8,\n",
       "  10,\n",
       "  11,\n",
       "  8,\n",
       "  12,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  2,\n",
       "  12,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  7,\n",
       "  12,\n",
       "  10,\n",
       "  8,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  8,\n",
       "  10,\n",
       "  12,\n",
       "  7,\n",
       "  9,\n",
       "  10,\n",
       "  12,\n",
       "  7,\n",
       "  7,\n",
       "  12,\n",
       "  7,\n",
       "  12,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  10,\n",
       "  3,\n",
       "  10,\n",
       "  7,\n",
       "  9,\n",
       "  5,\n",
       "  10,\n",
       "  5,\n",
       "  10,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  12,\n",
       "  10,\n",
       "  12,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  6,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  7,\n",
       "  12,\n",
       "  10,\n",
       "  12,\n",
       "  14,\n",
       "  11,\n",
       "  8,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  3,\n",
       "  11,\n",
       "  12,\n",
       "  7,\n",
       "  6,\n",
       "  12,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  10,\n",
       "  5,\n",
       "  10,\n",
       "  10,\n",
       "  12,\n",
       "  10,\n",
       "  10,\n",
       "  3,\n",
       "  10,\n",
       "  8,\n",
       "  12,\n",
       "  10,\n",
       "  8,\n",
       "  12,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  10,\n",
       "  10,\n",
       "  12,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  12,\n",
       "  11,\n",
       "  5,\n",
       "  12,\n",
       "  7,\n",
       "  8,\n",
       "  5,\n",
       "  13,\n",
       "  10,\n",
       "  11,\n",
       "  9,\n",
       "  10,\n",
       "  14,\n",
       "  12,\n",
       "  14,\n",
       "  7,\n",
       "  12,\n",
       "  11,\n",
       "  8,\n",
       "  14,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  11,\n",
       "  11,\n",
       "  6,\n",
       "  7,\n",
       "  12,\n",
       "  14,\n",
       "  13,\n",
       "  8,\n",
       "  12,\n",
       "  12,\n",
       "  6,\n",
       "  8,\n",
       "  12,\n",
       "  9,\n",
       "  2,\n",
       "  7,\n",
       "  9,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  10,\n",
       "  12,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  7,\n",
       "  10,\n",
       "  5,\n",
       "  9,\n",
       "  12,\n",
       "  7,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  9,\n",
       "  11,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  7,\n",
       "  10,\n",
       "  6,\n",
       "  10,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  6,\n",
       "  1,\n",
       "  7,\n",
       "  9,\n",
       "  10,\n",
       "  2,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  12,\n",
       "  10,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  7,\n",
       "  10,\n",
       "  8,\n",
       "  8,\n",
       "  6,\n",
       "  13,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  11,\n",
       "  9,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  11,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  5,\n",
       "  3,\n",
       "  12,\n",
       "  7,\n",
       "  10,\n",
       "  6,\n",
       "  10,\n",
       "  10,\n",
       "  6,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  10,\n",
       "  5,\n",
       "  10,\n",
       "  8,\n",
       "  5,\n",
       "  6,\n",
       "  11,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  7,\n",
       "  10,\n",
       "  5,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  10,\n",
       "  1,\n",
       "  6,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  7,\n",
       "  5,\n",
       "  10,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  10,\n",
       "  5,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  10,\n",
       "  7,\n",
       "  10,\n",
       "  5,\n",
       "  6,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  6,\n",
       "  5,\n",
       "  10,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  12,\n",
       "  14,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  3,\n",
       "  12,\n",
       "  7,\n",
       "  12,\n",
       "  5,\n",
       "  12,\n",
       "  6,\n",
       "  10,\n",
       "  10,\n",
       "  7,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  8,\n",
       "  10,\n",
       "  9,\n",
       "  2,\n",
       "  8,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  2,\n",
       "  11,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  7,\n",
       "  10,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  7,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  12,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  3,\n",
       "  12,\n",
       "  7,\n",
       "  7,\n",
       "  10,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  10,\n",
       "  12,\n",
       "  7,\n",
       "  5,\n",
       "  10,\n",
       "  8,\n",
       "  5,\n",
       "  10,\n",
       "  8,\n",
       "  5,\n",
       "  11,\n",
       "  6,\n",
       "  12,\n",
       "  5,\n",
       "  5,\n",
       "  10,\n",
       "  3,\n",
       "  10,\n",
       "  7,\n",
       "  9,\n",
       "  5,\n",
       "  10,\n",
       "  6,\n",
       "  10,\n",
       "  8,\n",
       "  12,\n",
       "  10,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  3,\n",
       "  11,\n",
       "  3,\n",
       "  6,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  2,\n",
       "  7,\n",
       "  10,\n",
       "  7,\n",
       "  2,\n",
       "  5,\n",
       "  8,\n",
       "  7,\n",
       "  3,\n",
       "  12,\n",
       "  7,\n",
       "  12,\n",
       "  5,\n",
       "  12,\n",
       "  7,\n",
       "  6,\n",
       "  8,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  5,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  5,\n",
       "  8,\n",
       "  7,\n",
       "  10,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  10,\n",
       "  5,\n",
       "  7,\n",
       "  10,\n",
       "  7,\n",
       "  7,\n",
       "  12,\n",
       "  5,\n",
       "  12,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  5,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  5,\n",
       "  10,\n",
       "  6,\n",
       "  7,\n",
       "  10,\n",
       "  5,\n",
       "  8,\n",
       "  2,\n",
       "  6,\n",
       "  11,\n",
       "  6,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  9,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  9,\n",
       "  10,\n",
       "  2,\n",
       "  12,\n",
       "  12,\n",
       "  7,\n",
       "  11,\n",
       "  8,\n",
       "  10,\n",
       "  12,\n",
       "  6,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  8,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  10,\n",
       "  8,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  10,\n",
       "  2,\n",
       "  12,\n",
       "  13,\n",
       "  7,\n",
       "  9,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  12,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  12,\n",
       "  5,\n",
       "  10,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  10,\n",
       "  11,\n",
       "  10,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  10,\n",
       "  6,\n",
       "  3,\n",
       "  10,\n",
       "  5,\n",
       "  6,\n",
       "  10,\n",
       "  5,\n",
       "  7,\n",
       "  10,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  10,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  4,\n",
       "  10,\n",
       "  8,\n",
       "  3,\n",
       "  10,\n",
       "  6,\n",
       "  5,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  12,\n",
       "  14,\n",
       "  6,\n",
       "  7,\n",
       "  11,\n",
       "  13,\n",
       "  5,\n",
       "  5,\n",
       "  10,\n",
       "  15,\n",
       "  10,\n",
       "  7,\n",
       "  9,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  12,\n",
       "  9,\n",
       "  8,\n",
       "  12,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  2,\n",
       "  5,\n",
       "  7,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  2,\n",
       "  5,\n",
       "  8,\n",
       "  6,\n",
       "  5,\n",
       "  8,\n",
       "  2,\n",
       "  11,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  8,\n",
       "  3,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  5,\n",
       "  8,\n",
       "  10,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  10,\n",
       "  12,\n",
       "  12,\n",
       "  10,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  6,\n",
       "  11,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  9,\n",
       "  11,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  5,\n",
       "  8,\n",
       "  9,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  10,\n",
       "  3,\n",
       "  5,\n",
       "  10,\n",
       "  5,\n",
       "  7,\n",
       "  2,\n",
       "  5,\n",
       "  11,\n",
       "  9,\n",
       "  5,\n",
       "  9,\n",
       "  3,\n",
       "  4,\n",
       "  10,\n",
       "  7,\n",
       "  12,\n",
       "  10,\n",
       "  7,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  6,\n",
       "  10,\n",
       "  8,\n",
       "  12,\n",
       "  5,\n",
       "  7,\n",
       "  10,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  9,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  14,\n",
       "  11,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  10,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  12,\n",
       "  7,\n",
       "  3,\n",
       "  5,\n",
       "  9,\n",
       "  10,\n",
       "  7,\n",
       "  11,\n",
       "  7,\n",
       "  5,\n",
       "  10,\n",
       "  9,\n",
       "  3,\n",
       "  8,\n",
       "  7,\n",
       "  12,\n",
       "  3,\n",
       "  7,\n",
       "  10,\n",
       "  11,\n",
       "  8,\n",
       "  10,\n",
       "  7,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  9,\n",
       "  8,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  7,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  9,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  13,\n",
       "  13,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  13,\n",
       "  12,\n",
       "  12,\n",
       "  14,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  12,\n",
       "  7,\n",
       "  14,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  14,\n",
       "  14,\n",
       "  11,\n",
       "  13,\n",
       "  14,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  12,\n",
       "  12,\n",
       "  14,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  12,\n",
       "  12,\n",
       "  14,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  13,\n",
       "  14,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  5,\n",
       "  12,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  12,\n",
       "  10,\n",
       "  12,\n",
       "  12,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  12,\n",
       "  10,\n",
       "  11,\n",
       "  10,\n",
       "  12,\n",
       "  10,\n",
       "  11,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  13,\n",
       "  12,\n",
       "  10,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  12,\n",
       "  10,\n",
       "  12,\n",
       "  10,\n",
       "  13,\n",
       "  12,\n",
       "  12,\n",
       "  10,\n",
       "  13,\n",
       "  12,\n",
       "  12,\n",
       "  ...],\n",
       " [10102950001,\n",
       "  10102950002,\n",
       "  10102950003,\n",
       "  10102950004,\n",
       "  10102950005,\n",
       "  10102950006,\n",
       "  10102950007,\n",
       "  10102950008,\n",
       "  10102950009,\n",
       "  10102950010,\n",
       "  10102950011,\n",
       "  10102950012,\n",
       "  10102950013,\n",
       "  10102950014,\n",
       "  10102950015,\n",
       "  10102950016,\n",
       "  10102950017,\n",
       "  10102950018,\n",
       "  10102950019,\n",
       "  10102950021,\n",
       "  10102950022,\n",
       "  10102950023,\n",
       "  10102950024,\n",
       "  10102950025,\n",
       "  10102950026,\n",
       "  10102950028,\n",
       "  10102950029,\n",
       "  10102950030,\n",
       "  10102950031,\n",
       "  10102950032,\n",
       "  10102950033,\n",
       "  10102950034,\n",
       "  10102950035,\n",
       "  10102950036,\n",
       "  10102950037,\n",
       "  10102950038,\n",
       "  10102950039,\n",
       "  10102950040,\n",
       "  10102950041,\n",
       "  10102950042,\n",
       "  10102950043,\n",
       "  10102960001,\n",
       "  10102960002,\n",
       "  10102960003,\n",
       "  10102960004,\n",
       "  10102960005,\n",
       "  10102960006,\n",
       "  10102960007,\n",
       "  10102960008,\n",
       "  10102960009,\n",
       "  10102960010,\n",
       "  10102960011,\n",
       "  10102960012,\n",
       "  10102960013,\n",
       "  10102960014,\n",
       "  10102970001,\n",
       "  10102970002,\n",
       "  10102970003,\n",
       "  10102970004,\n",
       "  10102970005,\n",
       "  10102970006,\n",
       "  10102970007,\n",
       "  10102970008,\n",
       "  10102970009,\n",
       "  10102970010,\n",
       "  10102970011,\n",
       "  10102970012,\n",
       "  10102970013,\n",
       "  10102970014,\n",
       "  10102970015,\n",
       "  10102970016,\n",
       "  10102970017,\n",
       "  10102970018,\n",
       "  10102970020,\n",
       "  10102970021,\n",
       "  10102970022,\n",
       "  10102970023,\n",
       "  10102970024,\n",
       "  10102970025,\n",
       "  10102970026,\n",
       "  10102970027,\n",
       "  10102970028,\n",
       "  10102970029,\n",
       "  10102970030,\n",
       "  10102970031,\n",
       "  10102970032,\n",
       "  10102970033,\n",
       "  10102970034,\n",
       "  10102970035,\n",
       "  10102970036,\n",
       "  10102970037,\n",
       "  10102970038,\n",
       "  10102970039,\n",
       "  10102970040,\n",
       "  10102970041,\n",
       "  10102970042,\n",
       "  10102970043,\n",
       "  10102970044,\n",
       "  10102980001,\n",
       "  10102980002,\n",
       "  10102980003,\n",
       "  10102980004,\n",
       "  10102980005,\n",
       "  10102980006,\n",
       "  10102980007,\n",
       "  10102980008,\n",
       "  10102980009,\n",
       "  10102980010,\n",
       "  10102980011,\n",
       "  10102980012,\n",
       "  10102980013,\n",
       "  10102980014,\n",
       "  10102980015,\n",
       "  10102980016,\n",
       "  10102980017,\n",
       "  10102990001,\n",
       "  10102990002,\n",
       "  10102990003,\n",
       "  10102990004,\n",
       "  10102990005,\n",
       "  10102990006,\n",
       "  10102990007,\n",
       "  10102990008,\n",
       "  10102990009,\n",
       "  10102990010,\n",
       "  10102990011,\n",
       "  10102990012,\n",
       "  10102990013,\n",
       "  10102990014,\n",
       "  10102990015,\n",
       "  10102990016,\n",
       "  10102990017,\n",
       "  10102990018,\n",
       "  10102990019,\n",
       "  10102990020,\n",
       "  10102990021,\n",
       "  10102990022,\n",
       "  10102990023,\n",
       "  10102990024,\n",
       "  10102990025,\n",
       "  10102990026,\n",
       "  10102990027,\n",
       "  10102990028,\n",
       "  10102990029,\n",
       "  10102990030,\n",
       "  10102990031,\n",
       "  10102990032,\n",
       "  10102990033,\n",
       "  10102990034,\n",
       "  10102990035,\n",
       "  10102990036,\n",
       "  10103000001,\n",
       "  10103000002,\n",
       "  10103000003,\n",
       "  10103000004,\n",
       "  10103000005,\n",
       "  10103000006,\n",
       "  10103000007,\n",
       "  10103000008,\n",
       "  10103000009,\n",
       "  10103000010,\n",
       "  10103000011,\n",
       "  10103000012,\n",
       "  10103000013,\n",
       "  10103000014,\n",
       "  10103000015,\n",
       "  10103000016,\n",
       "  10103000017,\n",
       "  10103000018,\n",
       "  10103000019,\n",
       "  10103000020,\n",
       "  10103000021,\n",
       "  10103000022,\n",
       "  10103000023,\n",
       "  10103000024,\n",
       "  10103000025,\n",
       "  10103000026,\n",
       "  10103000027,\n",
       "  10103000028,\n",
       "  10103000029,\n",
       "  10103000030,\n",
       "  10103000031,\n",
       "  10103010001,\n",
       "  10103010002,\n",
       "  10103010003,\n",
       "  10103010004,\n",
       "  10103010005,\n",
       "  10103010006,\n",
       "  10103010007,\n",
       "  10103010008,\n",
       "  10103010009,\n",
       "  10103010010,\n",
       "  10103010011,\n",
       "  10103010012,\n",
       "  10103010013,\n",
       "  10103010014,\n",
       "  10103010015,\n",
       "  10103010016,\n",
       "  10103010017,\n",
       "  10103010018,\n",
       "  10103010019,\n",
       "  10103010020,\n",
       "  10103010021,\n",
       "  10103010022,\n",
       "  10103010023,\n",
       "  10103010024,\n",
       "  10103010025,\n",
       "  10103010026,\n",
       "  10103010027,\n",
       "  10103010028,\n",
       "  10103010029,\n",
       "  10103010031,\n",
       "  10103010032,\n",
       "  10103020001,\n",
       "  10103020002,\n",
       "  10103020003,\n",
       "  10103020004,\n",
       "  10103020005,\n",
       "  10103020006,\n",
       "  10103020007,\n",
       "  10103020008,\n",
       "  10103020009,\n",
       "  10103020010,\n",
       "  10103020011,\n",
       "  10103020012,\n",
       "  10103020013,\n",
       "  10103020014,\n",
       "  10103020015,\n",
       "  10103020016,\n",
       "  10103020017,\n",
       "  10103020018,\n",
       "  10103020019,\n",
       "  10103020020,\n",
       "  10103020021,\n",
       "  10103020022,\n",
       "  10103020023,\n",
       "  10103020024,\n",
       "  10103030001,\n",
       "  10103030002,\n",
       "  10103030003,\n",
       "  10103030004,\n",
       "  10103030005,\n",
       "  10103030006,\n",
       "  10103030007,\n",
       "  10103030008,\n",
       "  10103030009,\n",
       "  10103030010,\n",
       "  10103030012,\n",
       "  10103030013,\n",
       "  10103030014,\n",
       "  10103030015,\n",
       "  10103030016,\n",
       "  10103030017,\n",
       "  10103030018,\n",
       "  10103030019,\n",
       "  10103030020,\n",
       "  10103030021,\n",
       "  10103030022,\n",
       "  10103030023,\n",
       "  10103030024,\n",
       "  10103030025,\n",
       "  10103030026,\n",
       "  10103030027,\n",
       "  10103030028,\n",
       "  10103030029,\n",
       "  10103030030,\n",
       "  10103030031,\n",
       "  10103030032,\n",
       "  10103030033,\n",
       "  10103030034,\n",
       "  10103030035,\n",
       "  10103030036,\n",
       "  10103030037,\n",
       "  10103040001,\n",
       "  10103040002,\n",
       "  10103040003,\n",
       "  10103040004,\n",
       "  10103040005,\n",
       "  10103040006,\n",
       "  10103040007,\n",
       "  10103040008,\n",
       "  10103040009,\n",
       "  10103040010,\n",
       "  10103040011,\n",
       "  10103040012,\n",
       "  10103040013,\n",
       "  10103040014,\n",
       "  10103050001,\n",
       "  10103050002,\n",
       "  10103050003,\n",
       "  10103050004,\n",
       "  10103050005,\n",
       "  10103050006,\n",
       "  10103050007,\n",
       "  10103050008,\n",
       "  10103050009,\n",
       "  10103050010,\n",
       "  10103050011,\n",
       "  10103050012,\n",
       "  10103050013,\n",
       "  10103050014,\n",
       "  10103050015,\n",
       "  10103050016,\n",
       "  10103050017,\n",
       "  10103050018,\n",
       "  10103050019,\n",
       "  10103050020,\n",
       "  10103050021,\n",
       "  10103050022,\n",
       "  10103050023,\n",
       "  10103050024,\n",
       "  10103050025,\n",
       "  10103050026,\n",
       "  10103050027,\n",
       "  10103050028,\n",
       "  10103050029,\n",
       "  10103050030,\n",
       "  10103050031,\n",
       "  10103050032,\n",
       "  10103050033,\n",
       "  10103050034,\n",
       "  10103050035,\n",
       "  10103050036,\n",
       "  10103050037,\n",
       "  10103050038,\n",
       "  10103050039,\n",
       "  10103050040,\n",
       "  10103050041,\n",
       "  10103050042,\n",
       "  10103050043,\n",
       "  10103050044,\n",
       "  10103050045,\n",
       "  10103050046,\n",
       "  10103050047,\n",
       "  10103050048,\n",
       "  10103050049,\n",
       "  10103050050,\n",
       "  10103050051,\n",
       "  10103050052,\n",
       "  10103050053,\n",
       "  10103050054,\n",
       "  10103050055,\n",
       "  10103050056,\n",
       "  10103050057,\n",
       "  10103050058,\n",
       "  10103050059,\n",
       "  10103050060,\n",
       "  10103050061,\n",
       "  10103050062,\n",
       "  10103050063,\n",
       "  10103050064,\n",
       "  10103050065,\n",
       "  10103050066,\n",
       "  10103050067,\n",
       "  10103050068,\n",
       "  10103050069,\n",
       "  10103050070,\n",
       "  10103050071,\n",
       "  10103050072,\n",
       "  10103050073,\n",
       "  10103050074,\n",
       "  10103050075,\n",
       "  10103050076,\n",
       "  10103050077,\n",
       "  10103050078,\n",
       "  10103050079,\n",
       "  10103050080,\n",
       "  10103050081,\n",
       "  10103050082,\n",
       "  10103050083,\n",
       "  10103050084,\n",
       "  10103050085,\n",
       "  10103050086,\n",
       "  10103050087,\n",
       "  10103050088,\n",
       "  10103060001,\n",
       "  10103060002,\n",
       "  10103060003,\n",
       "  10103060004,\n",
       "  10103060005,\n",
       "  10103060006,\n",
       "  10103060007,\n",
       "  10103060008,\n",
       "  10103060009,\n",
       "  10103060010,\n",
       "  10103060011,\n",
       "  10103060012,\n",
       "  10103060013,\n",
       "  10103060014,\n",
       "  10103060015,\n",
       "  10103060016,\n",
       "  10103070001,\n",
       "  10103070002,\n",
       "  10103070003,\n",
       "  10103070004,\n",
       "  10103070005,\n",
       "  10103070006,\n",
       "  10103070007,\n",
       "  10103070008,\n",
       "  10103070010,\n",
       "  10103070011,\n",
       "  10103070012,\n",
       "  10103070014,\n",
       "  10103070015,\n",
       "  10103070016,\n",
       "  10103070017,\n",
       "  10103070018,\n",
       "  10103070019,\n",
       "  10103070020,\n",
       "  10103070021,\n",
       "  10103070022,\n",
       "  10103070023,\n",
       "  10103070024,\n",
       "  10103070025,\n",
       "  10103070026,\n",
       "  10103070027,\n",
       "  10103070028,\n",
       "  10103070029,\n",
       "  10103070030,\n",
       "  10103070031,\n",
       "  10103070032,\n",
       "  10103070033,\n",
       "  10103070034,\n",
       "  10103070035,\n",
       "  10103070036,\n",
       "  10103070037,\n",
       "  10103070038,\n",
       "  10103070039,\n",
       "  10103070040,\n",
       "  10103070041,\n",
       "  10103070042,\n",
       "  10103080001,\n",
       "  10103080002,\n",
       "  10103080003,\n",
       "  10103080004,\n",
       "  10103080005,\n",
       "  10103080006,\n",
       "  10103080007,\n",
       "  10103080008,\n",
       "  10103080009,\n",
       "  10103080010,\n",
       "  10103080012,\n",
       "  10103080013,\n",
       "  10103080014,\n",
       "  10103080015,\n",
       "  10103080016,\n",
       "  10103080017,\n",
       "  10103080018,\n",
       "  10103080019,\n",
       "  10103090001,\n",
       "  10103090002,\n",
       "  10103090003,\n",
       "  10103090004,\n",
       "  10103090005,\n",
       "  10103090007,\n",
       "  10103090008,\n",
       "  10103090009,\n",
       "  10103090010,\n",
       "  10103090011,\n",
       "  10103090012,\n",
       "  10103090013,\n",
       "  10103090014,\n",
       "  10103090015,\n",
       "  10103090016,\n",
       "  10103090017,\n",
       "  10103090018,\n",
       "  10103090019,\n",
       "  10103100001,\n",
       "  10103100002,\n",
       "  10103100003,\n",
       "  10103100004,\n",
       "  10103100005,\n",
       "  10103100006,\n",
       "  10103100007,\n",
       "  10103100008,\n",
       "  10103100009,\n",
       "  10103100010,\n",
       "  10103100011,\n",
       "  10103100012,\n",
       "  10103100013,\n",
       "  10103100014,\n",
       "  10103100015,\n",
       "  10103100016,\n",
       "  10103100017,\n",
       "  10103100018,\n",
       "  10103100019,\n",
       "  10103100020,\n",
       "  10103100021,\n",
       "  10103100022,\n",
       "  10103100023,\n",
       "  10103100024,\n",
       "  10103100025,\n",
       "  10103100026,\n",
       "  10103100027,\n",
       "  10103100028,\n",
       "  10103100029,\n",
       "  10103100030,\n",
       "  10103100031,\n",
       "  10103100032,\n",
       "  10103100033,\n",
       "  10103100034,\n",
       "  10103100035,\n",
       "  10103100036,\n",
       "  10103100037,\n",
       "  10103100038,\n",
       "  10103100039,\n",
       "  10103100040,\n",
       "  10103100041,\n",
       "  10103110001,\n",
       "  10103110002,\n",
       "  10103110003,\n",
       "  10103110004,\n",
       "  10103110005,\n",
       "  10103110006,\n",
       "  10103110007,\n",
       "  10103110008,\n",
       "  10103110009,\n",
       "  10103110010,\n",
       "  10103110011,\n",
       "  10103110012,\n",
       "  10103110013,\n",
       "  10103110014,\n",
       "  10103110015,\n",
       "  10103110016,\n",
       "  10103110017,\n",
       "  10103110018,\n",
       "  10103110019,\n",
       "  10103110020,\n",
       "  10103110021,\n",
       "  10103110022,\n",
       "  10103110023,\n",
       "  10103110024,\n",
       "  10103110025,\n",
       "  10103110026,\n",
       "  10103110027,\n",
       "  10103110028,\n",
       "  10103110029,\n",
       "  10103110030,\n",
       "  10103110031,\n",
       "  10103110032,\n",
       "  10103110033,\n",
       "  10103110034,\n",
       "  10103110035,\n",
       "  10103110036,\n",
       "  10103110037,\n",
       "  10103110038,\n",
       "  10103110039,\n",
       "  10103110040,\n",
       "  10103110041,\n",
       "  10103110042,\n",
       "  10103110043,\n",
       "  10103110044,\n",
       "  10103110045,\n",
       "  10103110046,\n",
       "  10103110047,\n",
       "  10103110048,\n",
       "  10103110049,\n",
       "  10103110050,\n",
       "  10103110051,\n",
       "  10103110052,\n",
       "  10103110053,\n",
       "  10103110054,\n",
       "  10103110055,\n",
       "  10103110056,\n",
       "  10103110057,\n",
       "  10103110058,\n",
       "  10103120001,\n",
       "  10103120002,\n",
       "  10103120003,\n",
       "  10103120004,\n",
       "  10103120005,\n",
       "  10103120006,\n",
       "  10103120007,\n",
       "  10103120008,\n",
       "  10103120009,\n",
       "  10103120010,\n",
       "  10103120011,\n",
       "  10103120012,\n",
       "  10103120013,\n",
       "  10103120014,\n",
       "  10103120015,\n",
       "  10103120016,\n",
       "  10103120017,\n",
       "  10103120018,\n",
       "  10103130001,\n",
       "  10103130002,\n",
       "  10103130003,\n",
       "  10103130004,\n",
       "  10103130005,\n",
       "  10103130006,\n",
       "  10103130007,\n",
       "  10103130008,\n",
       "  10103130009,\n",
       "  10103130010,\n",
       "  10103130011,\n",
       "  10103130012,\n",
       "  10103130013,\n",
       "  10103130014,\n",
       "  10103130015,\n",
       "  10103130016,\n",
       "  10103130017,\n",
       "  10103130018,\n",
       "  10103130019,\n",
       "  10103130021,\n",
       "  10103130022,\n",
       "  10103130023,\n",
       "  10103130024,\n",
       "  10103130025,\n",
       "  10103130026,\n",
       "  10103130027,\n",
       "  10103130028,\n",
       "  10103140002,\n",
       "  10103140003,\n",
       "  10103140004,\n",
       "  10103140005,\n",
       "  10103140006,\n",
       "  10103140007,\n",
       "  10103140008,\n",
       "  10103140009,\n",
       "  10103140010,\n",
       "  10103140011,\n",
       "  10103140012,\n",
       "  10103140013,\n",
       "  10103140014,\n",
       "  10103140015,\n",
       "  10103140016,\n",
       "  10103140017,\n",
       "  10103140018,\n",
       "  10103140019,\n",
       "  10103140020,\n",
       "  10103140021,\n",
       "  10103140022,\n",
       "  10103140023,\n",
       "  10103140024,\n",
       "  10103140025,\n",
       "  10103140026,\n",
       "  10103140027,\n",
       "  10103140028,\n",
       "  10103140029,\n",
       "  10103140030,\n",
       "  10103140031,\n",
       "  10103140032,\n",
       "  10103150001,\n",
       "  10103150002,\n",
       "  10103150003,\n",
       "  10103150004,\n",
       "  10103150005,\n",
       "  10103150006,\n",
       "  10103150007,\n",
       "  10103150008,\n",
       "  10103150009,\n",
       "  10103150010,\n",
       "  10103150011,\n",
       "  10103150012,\n",
       "  10103150013,\n",
       "  10103150014,\n",
       "  10103150015,\n",
       "  10103150016,\n",
       "  10103150017,\n",
       "  10103150018,\n",
       "  10103150019,\n",
       "  10103150020,\n",
       "  10103150021,\n",
       "  10103150022,\n",
       "  10103150023,\n",
       "  10103150024,\n",
       "  10103150025,\n",
       "  10103150026,\n",
       "  10103150027,\n",
       "  10103150028,\n",
       "  10103150029,\n",
       "  10103150030,\n",
       "  10103150031,\n",
       "  10103150032,\n",
       "  10103150033,\n",
       "  10103150034,\n",
       "  10103150035,\n",
       "  10103150036,\n",
       "  10103150037,\n",
       "  10103150038,\n",
       "  10103150039,\n",
       "  10103150040,\n",
       "  10103150041,\n",
       "  10103150042,\n",
       "  10103160001,\n",
       "  10103160002,\n",
       "  10103160003,\n",
       "  10103160004,\n",
       "  10103160005,\n",
       "  10103160006,\n",
       "  10103160008,\n",
       "  10103160009,\n",
       "  10103160010,\n",
       "  10103160011,\n",
       "  10103160012,\n",
       "  10103160013,\n",
       "  10103160014,\n",
       "  10103160015,\n",
       "  10103160016,\n",
       "  10103160017,\n",
       "  10103160018,\n",
       "  10103160019,\n",
       "  10103160020,\n",
       "  10103160021,\n",
       "  10103160022,\n",
       "  10103160023,\n",
       "  10103160024,\n",
       "  10103160025,\n",
       "  10103160026,\n",
       "  10103160027,\n",
       "  10103160028,\n",
       "  10103160029,\n",
       "  10103160030,\n",
       "  10103160031,\n",
       "  10103160032,\n",
       "  10103160033,\n",
       "  10103160034,\n",
       "  10103160035,\n",
       "  10103160036,\n",
       "  10103160037,\n",
       "  10103160038,\n",
       "  10103160039,\n",
       "  10103160040,\n",
       "  10103160041,\n",
       "  10103160042,\n",
       "  10103160043,\n",
       "  10103160044,\n",
       "  10103160045,\n",
       "  10103160046,\n",
       "  10103160047,\n",
       "  10103160048,\n",
       "  10103160049,\n",
       "  10103160050,\n",
       "  10103160051,\n",
       "  10103160052,\n",
       "  10103160053,\n",
       "  10103160054,\n",
       "  10103160055,\n",
       "  10103160056,\n",
       "  10103160058,\n",
       "  10103160059,\n",
       "  10103160060,\n",
       "  10103160061,\n",
       "  10103160062,\n",
       "  10103160063,\n",
       "  10103160064,\n",
       "  10103160065,\n",
       "  10103160066,\n",
       "  10103160067,\n",
       "  10103160068,\n",
       "  10103160069,\n",
       "  10103160070,\n",
       "  10103160071,\n",
       "  10103160072,\n",
       "  10103160073,\n",
       "  10103160074,\n",
       "  10103160075,\n",
       "  10103160076,\n",
       "  10103160077,\n",
       "  10103160078,\n",
       "  10103160079,\n",
       "  10103160080,\n",
       "  10103160081,\n",
       "  10103160082,\n",
       "  10103160083,\n",
       "  10103160084,\n",
       "  10103160085,\n",
       "  10103160086,\n",
       "  10103160087,\n",
       "  10103160088,\n",
       "  10103160089,\n",
       "  10103160090,\n",
       "  10103160091,\n",
       "  10103160092,\n",
       "  10103160093,\n",
       "  10103160094,\n",
       "  10103160095,\n",
       "  10103160096,\n",
       "  10103160097,\n",
       "  10103160098,\n",
       "  10103160099,\n",
       "  10103160100,\n",
       "  10103160101,\n",
       "  10103160102,\n",
       "  10103160103,\n",
       "  10103160104,\n",
       "  10103160105,\n",
       "  10103160106,\n",
       "  10103160107,\n",
       "  10103160108,\n",
       "  10103160109,\n",
       "  10103160110,\n",
       "  10103160111,\n",
       "  10103160112,\n",
       "  10103160113,\n",
       "  10103160114,\n",
       "  10103160115,\n",
       "  10103160116,\n",
       "  10103160118,\n",
       "  10103160119,\n",
       "  10103160120,\n",
       "  10103160121,\n",
       "  10103160122,\n",
       "  10103160123,\n",
       "  10103160124,\n",
       "  10103160125,\n",
       "  10103160126,\n",
       "  10103160127,\n",
       "  10103160128,\n",
       "  10103160129,\n",
       "  10103160130,\n",
       "  10103160131,\n",
       "  10103160132,\n",
       "  10103160133,\n",
       "  10103160134,\n",
       "  10103160135,\n",
       "  10103160136,\n",
       "  10103160137,\n",
       "  10103160138,\n",
       "  10103160139,\n",
       "  10103160141,\n",
       "  10103160142,\n",
       "  10103160143,\n",
       "  10103160144,\n",
       "  10103160145,\n",
       "  10103160146,\n",
       "  10103160147,\n",
       "  10103160148,\n",
       "  10103160149,\n",
       "  10103160150,\n",
       "  10103160151,\n",
       "  10103170002,\n",
       "  10103170003,\n",
       "  10103170004,\n",
       "  10103170005,\n",
       "  10103170006,\n",
       "  10103170007,\n",
       "  10103170008,\n",
       "  10103170009,\n",
       "  10103170010,\n",
       "  10103170011,\n",
       "  10103170012,\n",
       "  10103170013,\n",
       "  10103170015,\n",
       "  10103170016,\n",
       "  10103170017,\n",
       "  10103170018,\n",
       "  10103170019,\n",
       "  10103170020,\n",
       "  10103180001,\n",
       "  10103180002,\n",
       "  10103180003,\n",
       "  10103180004,\n",
       "  10103180005,\n",
       "  10103180006,\n",
       "  10103180007,\n",
       "  10103180008,\n",
       "  10103180009,\n",
       "  10103180010,\n",
       "  10103180011,\n",
       "  10103180012,\n",
       "  10103180014,\n",
       "  10103180015,\n",
       "  10103180016,\n",
       "  10103180017,\n",
       "  10103180018,\n",
       "  10103180019,\n",
       "  10103180020,\n",
       "  10103180021,\n",
       "  10103180022,\n",
       "  10103180023,\n",
       "  10103180024,\n",
       "  10103180025,\n",
       "  10103180026,\n",
       "  10103180027,\n",
       "  10103180028,\n",
       "  10103180029,\n",
       "  10103180030,\n",
       "  10103180031,\n",
       "  10103180032,\n",
       "  10103180033,\n",
       "  10200240001,\n",
       "  10200240003,\n",
       "  10200240004,\n",
       "  10200240005,\n",
       "  10200240006,\n",
       "  10200240007,\n",
       "  10200240008,\n",
       "  10200240009,\n",
       "  10200240010,\n",
       "  10200240011,\n",
       "  10200240012,\n",
       "  10200240013,\n",
       "  10200240014,\n",
       "  10200240015,\n",
       "  10200240017,\n",
       "  10200240019,\n",
       "  10200250001,\n",
       "  10200250002,\n",
       "  10200250003,\n",
       "  10200250004,\n",
       "  10200250005,\n",
       "  10200250006,\n",
       "  10200250007,\n",
       "  10200250008,\n",
       "  10200250009,\n",
       "  10200250010,\n",
       "  10200250011,\n",
       "  10200250012,\n",
       "  10200250013,\n",
       "  10200250014,\n",
       "  10200250016,\n",
       "  10200250017,\n",
       "  10200250018,\n",
       "  10200250019,\n",
       "  10200250020,\n",
       "  10200260001,\n",
       "  10200260002,\n",
       "  10200260003,\n",
       "  10200260004,\n",
       "  10200260005,\n",
       "  10200260006,\n",
       "  10200260007,\n",
       "  10200260008,\n",
       "  10200260009,\n",
       "  10200260010,\n",
       "  10200260011,\n",
       "  10200260012,\n",
       "  10200260013,\n",
       "  10200260014,\n",
       "  10200260015,\n",
       "  10200260016,\n",
       "  10200260017,\n",
       "  10200260018,\n",
       "  10200260019,\n",
       "  10200260020,\n",
       "  10200270001,\n",
       "  10200270002,\n",
       "  10200270003,\n",
       "  10200270004,\n",
       "  10200270006,\n",
       "  10200270007,\n",
       "  10200270008,\n",
       "  10200270009,\n",
       "  10200270010,\n",
       "  10200270011,\n",
       "  10200270012,\n",
       "  10200270013,\n",
       "  10200270016,\n",
       "  10200270017,\n",
       "  10200270018,\n",
       "  10200270019,\n",
       "  10200270021,\n",
       "  10200270022,\n",
       "  10200270023,\n",
       "  10300160001,\n",
       "  10300160002,\n",
       "  10300160003,\n",
       "  10300160004,\n",
       "  10300160005,\n",
       "  10300160006,\n",
       "  10300160007,\n",
       "  10300160008,\n",
       "  10300160009,\n",
       "  10300160010,\n",
       "  10300160011,\n",
       "  10300160012,\n",
       "  10300160013,\n",
       "  10300160014,\n",
       "  10300160015,\n",
       "  10300160016,\n",
       "  10300160017,\n",
       "  10300160018,\n",
       "  10300160019,\n",
       "  10300160020,\n",
       "  10300160021,\n",
       "  10300160022,\n",
       "  10300160023,\n",
       "  10300160024,\n",
       "  10300160025,\n",
       "  10300160026,\n",
       "  10300160027,\n",
       "  10300160028,\n",
       "  10300160029,\n",
       "  10300160030,\n",
       "  10300160031,\n",
       "  10300160032,\n",
       "  10300160033,\n",
       "  10300160034,\n",
       "  10300160035,\n",
       "  10300160036,\n",
       "  10300160037,\n",
       "  10300160038,\n",
       "  10300160039,\n",
       "  10300160040,\n",
       "  10300160041,\n",
       "  10300160042,\n",
       "  10300160043,\n",
       "  10300160044,\n",
       "  10300160045,\n",
       "  ...])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output_to_file(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_ids = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            # dummy: set real labels all = 0\n",
    "            if hasattr(data, 'y'):\n",
    "                data.y = torch.zeros_like(data.y, device=data.y.device, dtype=torch.long)\n",
    "\n",
    "            out = model(data)\n",
    "            logits = out[\"logits\"]\n",
    "\n",
    "            # Use coral decoding or CE depending on your training\n",
    "            preds = coral_decode(logits)\n",
    "            preds = [int(pred) + 1 for pred in preds]\n",
    "            all_preds.extend(preds)\n",
    "            all_ids.extend(data.id.cpu().numpy())  # assumes sentence_id exists\n",
    "\n",
    "    # Save to file named exactly \"prediction\" (no .csv)\n",
    "    with open(\"prediction\", \"w\") as f:\n",
    "        f.write(\"Sentence ID,Prediction\\n\")\n",
    "        for sid, pred in zip(all_ids, all_preds):\n",
    "            f.write(f\"{sid},{pred}\\n\")\n",
    "\n",
    "    return all_preds, all_ids\n",
    "\n",
    "model.load_state_dict(torch.load('model_best_qwk.pt'))\n",
    "# model.load_state_dict(torch.load('model_best_acc.pt'))  # or model_best_qwk.pt\n",
    "output_to_file(model, blind_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some experiments, you dont need to continue anymore, was checking the effectiveness of syntactic, morphological features, All work here is unreported in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo1fWJOV2AHt"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_vocab_for_sequence_feature(dataset, feature_name):\n",
    "    all_tokens = []\n",
    "    for example in dataset:\n",
    "        if feature_name in example and example[feature_name]:\n",
    "            tokens = example[feature_name].split()\n",
    "            all_tokens.extend(tokens)\n",
    "    unique_tokens = sorted(set(all_tokens))\n",
    "    return {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "\n",
    "def build_vocab_for_all_features(dataset, feature_names):\n",
    "    feature_vocabs = {}\n",
    "    for feat in feature_names:\n",
    "        vocab = build_vocab_for_sequence_feature(dataset, feat)\n",
    "        feature_vocabs[feat] = vocab\n",
    "    return feature_vocabs\n",
    "\n",
    "\n",
    "feature_vocabs = build_vocab_for_all_features(dataset, morph_features)\n",
    "\n",
    "# Optional: print vocab sizes\n",
    "for feat, vocab in feature_vocabs.items():\n",
    "    print(f\"{feat}: {len(vocab)} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feat = \"Sentence\"\n",
    "\n",
    "lengths = [len(sent.split()) for sent in dataset[feat]]\n",
    "max_len_sent_95 = int(np.percentile(lengths, 95))\n",
    "\n",
    "print(f\"Feature: {feat}\")\n",
    "print(f\"Min length: {min(lengths)}\")\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"Mean length: {sum(lengths) / len(lengths):.2f}\")\n",
    "print(f\"95th percentile length for {feat}: {max_len_sent_95}\")\n",
    "\n",
    "plt.hist(lengths, bins=30, alpha=0.7)\n",
    "plt.xlabel(\"Length of Sentence feature tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Length distribution for {feat}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Do the same for Morph feature \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feat = morph_features[0]  # Pick one morphological feature, since all should be the same\n",
    "\n",
    "lengths = [len(morph_str.split()) for morph_str in dataset[feat]]\n",
    "max_len_morph_95 = int(np.percentile(lengths, 95))\n",
    "\n",
    "print(f\"Feature: {feat}\")\n",
    "print(f\"Min length: {min(lengths)}\")\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"Mean length: {sum(lengths) / len(lengths):.2f}\")\n",
    "print(f\"95th percentile length for {feat}: {max_len_morph_95}\")\n",
    "\n",
    "plt.hist(lengths, bins=30, alpha=0.7)\n",
    "plt.xlabel(\"Length of morphological feature tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Length distribution for {feat}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MC3_840FxJPW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"aubmindlab/bert-base-arabertv02\"\n",
    "# model_ckpt = \"CAMeL-Lab/readability-arabertv02-word-CE\"\n",
    "# model_ckpt = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# Tokenization\n",
    "# def tokenize(batch):\n",
    "#     return tokenizer(batch[\"Sentence\"], batch[\"pos_string\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# try to make morph feature embedding\n",
    "def morph_str_to_ids(feature_name, morph_string):\n",
    "    return [feature_vocabs[feature_name][tag] for tag in morph_string.split()]\n",
    "\n",
    "def tokenize(batch, feature_names=morph_features, max_len=128):\n",
    "    # Tokenize the sentence\n",
    "    tokenized = tokenizer(batch[\"Sentence\"], padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "\n",
    "    # For each morph feature, convert to ID list, pad/truncate, and store\n",
    "    for feat in feature_names:\n",
    "        all_ids = []\n",
    "        for morph_str in batch[feat]:\n",
    "            ids = morph_str_to_ids(feat, morph_str)\n",
    "            if len(ids) > max_len:\n",
    "                ids = ids[:max_len]\n",
    "            else:\n",
    "                ids = ids + [0] * (max_len - len(ids))  # pad with 0\n",
    "            all_ids.append(ids)\n",
    "        tokenized[f\"{feat}_ids\"] = all_ids\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "morph_columns = [f\"{feat}_ids\" for feat in morph_features]\n",
    "torch_columns = [\"input_ids\", \"attention_mask\", \"labels\", \"syntactic_feats\"] + morph_columns\n",
    "\n",
    "# Set torch format for syntactic_feats to be correctly read by collator\n",
    "encoded_dataset.set_format(\"torch\", columns=torch_columns, output_all_columns=True)\n",
    "encoded_eval_dataset.set_format(\"torch\", columns=torch_columns, output_all_columns=True)\n",
    "encoded_test_dataset.set_format(\"torch\", columns=torch_columns, output_all_columns=True)\n",
    "\n",
    "# Check\n",
    "type(encoded_dataset[\"syntactic_feats\"][0]), type(encoded_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(encoded_dataset[\"input_ids\"][0].shape)\n",
    "print(type(encoded_dataset[\"syntactic_feats\"][0]), type(encoded_dataset[\"input_ids\"][0]), type(encoded_dataset[\"ud_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8P_ojgUxJPW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOpnhgULxJPW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=19).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTbrjf46BNpS"
   },
   "source": [
    "## Ordinal instead of AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PdIvXPeBIgF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertPreTrainedModel, AutoConfig\n",
    "from coral_pytorch.losses import coral_loss\n",
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "\n",
    "\n",
    "class BertCoralOrdinal(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "            nn.Dropout(config.hidden_dropout_prob),\n",
    "            nn.Linear(config.hidden_size, num_labels - 1)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)\n",
    "        logits = self.classifier(pooled_output)  # shape: [batch_size, num_labels - 1]\n",
    "\n",
    "        if labels is not None:\n",
    "            # Convert labels to CORAL ordinal levels (shape: [batch_size, num_labels - 1])\n",
    "            levels = levels_from_labelbatch(labels, self.num_labels).to(logits.device)\n",
    "            loss = coral_loss(logits, levels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        else:\n",
    "            return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOwB2biWCDF5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "num_labels = 19\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "model = BertCoralOrdinal.from_pretrained(model_ckpt, config=config, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Coral With Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertPreTrainedModel, AutoConfig\n",
    "from coral_pytorch.losses import coral_loss\n",
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "\n",
    "class BertCoralOrdinalWithFeatures(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, feature_dim):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # Combine BERT + syntactic features\n",
    "        combined_dim = config.hidden_size + feature_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "            nn.Dropout(config.hidden_dropout_prob),\n",
    "            nn.Linear(config.hidden_size, num_labels - 1)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, syntactic_feats=None, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)\n",
    "        \n",
    "        if syntactic_feats is not None:\n",
    "            # [batch_size, feature_dim]\n",
    "            combined = torch.cat([pooled_output, syntactic_feats], dim=1)  # Concatenate along feature axis\n",
    "        else:\n",
    "            print(\"no syntactic_feats\")\n",
    "            combined = pooled_output\n",
    "            \n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Convert labels to CORAL ordinal levels (shape: [batch_size, num_labels - 1])\n",
    "            levels = levels_from_labelbatch(labels, self.num_labels).to(logits.device)\n",
    "            loss = coral_loss(logits, levels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        else:\n",
    "            return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "num_labels = 19\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "model = BertCoralOrdinalWithFeatures.from_pretrained(model_ckpt, config=config, num_labels=num_labels, feature_dim=len(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Coral Bert With Features and All Morph's embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "class BertWithMorpholoEmbeddings(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, morph_vocab_sizes, feature_dim, morph_emb_dim=16):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.feature_dim = feature_dim\n",
    "        self.morph_emb_dim = morph_emb_dim\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # Create a separate embedding layer for each morphological feature\n",
    "        self.morph_embeddings = nn.ModuleDict({\n",
    "            feat: nn.Embedding(vocab_size, morph_emb_dim)\n",
    "            for feat, vocab_size in morph_vocab_sizes.items()\n",
    "        })\n",
    "\n",
    "        # Total morph embedding dimension = morph_emb_dim * number of morph features\n",
    "        self.total_morph_dim = morph_emb_dim * len(morph_vocab_sizes)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        combined_dim = config.hidden_size + self.total_morph_dim + feature_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "            nn.Dropout(config.hidden_dropout_prob),\n",
    "            nn.Linear(config.hidden_size, num_labels - 1)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Call init_weights only if using transformers.BertPreTrainedModel's pattern\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                syntactic_feats=None,\n",
    "                labels=None,\n",
    "                ud_ids=None,\n",
    "                prc3_ids=None,\n",
    "                prc2_ids=None,\n",
    "                prc1_ids=None,\n",
    "                prc0_ids=None,\n",
    "                enc0_ids=None,\n",
    "                gen_ids=None,\n",
    "                num_ids=None,\n",
    "                cas_ids=None,\n",
    "                per_ids=None,\n",
    "                asp_ids=None,\n",
    "                vox_ids=None,\n",
    "                mod_ids=None,\n",
    "                stt_ids=None,\n",
    "                rat_ids=None,\n",
    "                token_type_ids=None):\n",
    "\n",
    "        # Prepare a dict with all morph inputs\n",
    "        morph_inputs = {\n",
    "            \"ud_ids\": ud_ids,\n",
    "            \"prc3_ids\": prc3_ids,\n",
    "            \"prc2_ids\": prc2_ids,\n",
    "            \"prc1_ids\": prc1_ids,\n",
    "            \"prc0_ids\": prc0_ids,\n",
    "            \"enc0_ids\": enc0_ids,\n",
    "            \"gen_ids\": gen_ids,\n",
    "            \"num_ids\": num_ids,\n",
    "            \"cas_ids\": cas_ids,\n",
    "            \"per_ids\": per_ids,\n",
    "            \"asp_ids\": asp_ids,\n",
    "            \"vox_ids\": vox_ids,\n",
    "            \"mod_ids\": mod_ids,\n",
    "            \"stt_ids\": stt_ids,\n",
    "            \"rat_ids\": rat_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "\n",
    "        # Filter out None values (if some features are optional)\n",
    "        morph_inputs = {k: v for k, v in morph_inputs.items() if v is not None}\n",
    "\n",
    "        # Pass BERT inputs (exclude morph features)\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "    \n",
    "        morph_embeds = []\n",
    "        for feat_name, embed_layer in self.morph_embeddings.items():\n",
    "            key = feat_name + \"_ids\"\n",
    "            if key not in morph_inputs:\n",
    "                raise ValueError(f\"Missing {key} in input to forward()\")\n",
    "            feat_ids = morph_inputs[key]\n",
    "            morph_embeds.append(embed_layer(feat_ids))  # [batch, seq_len, morph_emb_dim]\n",
    "\n",
    "        combined_feats = torch.cat(morph_embeds, dim=-1)  # [batch, seq_len, morph_emb_dim * num_feats]\n",
    "    \n",
    "        # Combine BERT embeddings with morph embeddings\n",
    "        combined = torch.cat([token_embeddings, combined_feats], dim=-1)\n",
    "    \n",
    "        # Masked mean pooling\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(combined.size()).float()\n",
    "        summed = torch.sum(combined * attention_mask_expanded, dim=1)\n",
    "        summed_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "        pooled = summed / summed_mask  # [batch_size, combined_dim]\n",
    "    \n",
    "        pooled = self.dropout(pooled)\n",
    "    \n",
    "        # Add syntactic_feats if provided\n",
    "        if syntactic_feats is not None:\n",
    "            combined = torch.cat([pooled, syntactic_feats], dim=1)\n",
    "        else:\n",
    "            combined = pooled\n",
    "    \n",
    "        logits = self.classifier(combined)\n",
    "    \n",
    "        if labels is not None:\n",
    "            levels = levels_from_labelbatch(labels, self.num_labels).to(logits.device)\n",
    "            loss = coral_loss(logits, levels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        else:\n",
    "            return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "morph_vocab_sizes = {feat: len(vocab) for feat, vocab in feature_vocabs.items()}\n",
    "feature_dim = len(feature_columns)  # e.g., if you're using handcrafted syntactic features\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "model = BertWithMorpholoEmbeddings.from_pretrained(\n",
    "    model_ckpt,\n",
    "    config=config,\n",
    "    num_labels=num_labels,\n",
    "    feature_dim=feature_dim,\n",
    "    morph_vocab_sizes=morph_vocab_sizes,\n",
    "    morph_emb_dim=16  # or any other dimension you want\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qsDaxaWU9fY"
   },
   "source": [
    "# Try Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqT-eWplVYPG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BertRegression(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "            nn.Dropout(config.hidden_dropout_prob),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)\n",
    "        logits = self.classifier(pooled_output).squeeze(-1)  # shape: [batch_size]\n",
    "\n",
    "        if labels is not None:\n",
    "          labels = labels.float()  # Important for regression\n",
    "          loss = nn.SmoothL1Loss()(logits, labels) # SmoothL1Loss try too\n",
    "          return {\"loss\": loss, \"logits\": logits}\n",
    "        else:\n",
    "          return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXOie7VDVoQZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "num_labels = 19\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "model = BertRegression.from_pretrained(model_ckpt, config=config, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert + syntactical Features ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BertWithFeaturesRegression(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, feature_dim):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # Combine BERT + syntactic features\n",
    "        combined_dim = config.hidden_size + feature_dim\n",
    "        # print(\"config.hidden_size\", config.hidden_size)\n",
    "        # print(\"feature dim \", feature_dim)\n",
    "        # print(\"combined_dim \", combined_dim)\n",
    "        \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "            nn.Dropout(config.hidden_dropout_prob),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, syntactic_feats=None, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)  # [batch_size, hidden_size] = [batch_size, 768]\n",
    "\n",
    "        if syntactic_feats is not None:\n",
    "            # [batch_size, feature_dim]\n",
    "            combined = torch.cat([pooled_output, syntactic_feats], dim=1)  # Concatenate along feature axis\n",
    "        else:\n",
    "            print(\"no syntactic_feats\")\n",
    "            combined = pooled_output\n",
    "\n",
    "        logits = self.classifier(combined).squeeze(-1)  # [batch_size]\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            loss = nn.SmoothL1Loss()(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        else:\n",
    "            return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "num_labels = 19\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "model = BertWithFeaturesRegression.from_pretrained(model_ckpt, config=config, num_labels=num_labels, feature_dim=len(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decode\n",
    "input_ids = encoded_dataset[0]['input_ids']\n",
    "sentence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "morph_vocab_sizes = {feat: len(vocab) for feat, vocab in feature_vocabs.items()}\n",
    "feature_dim = len(feature_columns)  # e.g., if you're using handcrafted syntactic features\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "model = BertWithMorpholoEmbeddings.from_pretrained(\n",
    "    model_ckpt,\n",
    "    config=config,\n",
    "    num_labels=num_labels,\n",
    "    feature_dim=feature_dim,\n",
    "    morph_vocab_sizes=morph_vocab_sizes,\n",
    "    morph_emb_dim=16  # or any other dimension you want\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "def custom_data_collator(features):\n",
    "    # Use HuggingFace's default collator to handle input_ids, attention_mask, labels, etc.\n",
    "    batch = default_data_collator(features)\n",
    "\n",
    "    # Stack syntactic features manually\n",
    "    if \"syntactic_feats\" in features[0]:\n",
    "        batch[\"syntactic_feats\"] = torch.stack([f[\"syntactic_feats\"] for f in features])\n",
    "\n",
    "    # Handle morph features like ud_ids, gen_ids, etc.\n",
    "    for feat in morph_features:\n",
    "        key = f\"{feat}_ids\"\n",
    "        if key in features[0]:\n",
    "            batch[key] = torch.stack([f[key] for f in features])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP-JRmnSCA_x"
   },
   "source": [
    "# Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gShshQfxJPX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./barec_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    gradient_accumulation_steps=1,   # simulate bigger batch if needed\n",
    "    dataloader_num_workers=4,        # speed up data loading\n",
    "    fp16=True,                       # enable mixed precision for speed/memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ol0aTz5yxJPX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, cohen_kappa_score\n",
    "\n",
    "def coral_decode(logits):\n",
    "    \"\"\"\n",
    "    logits: [batch_size, num_classes-1] tensor\n",
    "    returns: numpy array of predicted class labels\n",
    "    \"\"\"\n",
    "    probas = torch.sigmoid(torch.tensor(logits))  # sigmoid over logits\n",
    "    # For each sample, count how many thresholds are > 0.5\n",
    "    preds = (probas > 0.5).sum(dim=1).cpu().numpy()\n",
    "    return preds\n",
    "\n",
    "def regression_decode(logits, labels):\n",
    "    #  Flatten predictions and labels if needed\n",
    "    logits = logits.squeeze()\n",
    "    labels = labels.squeeze()\n",
    "\n",
    "    def round_and_clip(logits, labels):\n",
    "        rounded = np.round(logits).astype(int)\n",
    "        return np.clip(rounded, 0, num_labels - 1)\n",
    "\n",
    "    # Get rounded predictions for classification-based metrics\n",
    "    return round_and_clip(logits, labels)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # 1. Classification\n",
    "    # preds = np.argmax(logits, axis=1)\n",
    "    # 2. Coral\n",
    "    preds = coral_decode(logits)  # decode ordinal logits into class preds\n",
    "    # 3. Regression\n",
    "    # preds = regression_decode(logits, labels)\n",
    "\n",
    "    acc = (preds == labels).mean()\n",
    "    qwk = cohen_kappa_score(labels, preds, weights='quadratic')\n",
    "    return {\"accuracy\": acc, \"qwk\": qwk}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # TODO:: added custom_data_collator for our new arch\n",
    "    data_collator=custom_data_collator,  # 👈 use custom collator here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnX9nOJWxJPX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93IMTtrvxJPX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UH2s6t56olD"
   },
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JGTYzqzMUOU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def coral_predict_labels(logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert CORAL logits to predicted ordinal labels.\n",
    "    The logits are shape (batch_size, num_labels - 1).\n",
    "    Each logit represents the log-odds of being above a class threshold.\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(torch.tensor(logits))  # apply sigmoid to get probabilities\n",
    "    cum_preds = probs > 0.5                      # boolean mask: passed threshold?\n",
    "    labels = cum_preds.sum(dim=1).numpy()        # count how many thresholds passed\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eylcEK_sxJPX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = trainer.predict(encoded_test_dataset)\n",
    "\n",
    "# 1. Classification\n",
    "# pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "# 2. Coral\n",
    "pred_labels = coral_predict_labels(preds.predictions)\n",
    "\n",
    "# 3. Regression\n",
    "# raw_preds = preds.predictions.squeeze()\n",
    "# pred_labels = np.clip(np.round(raw_preds), 0, num_labels - 1).astype(int)\n",
    "\n",
    "# Add one to the label\n",
    "pred_labels = pred_labels + 1\n",
    "\n",
    "# Match format expected by evaluation script\n",
    "eval_ids = [str(x) for x in encoded_test_dataset[\"ID\"]]\n",
    "submission = pd.DataFrame({\n",
    "    \"Sentence ID\": eval_ids,\n",
    "    \"Prediction\": pred_labels\n",
    "})\n",
    "submission.to_csv(\"Test_Sentence_Level.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC-ZHaIw4glf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "barec_7_dict = {1: 1, 2: 1, 3: 1, 4: 1, 5: 2, 6: 2, 7: 2, 8: 3, 9: 3, 10: 4, 11: 4, 12: 5, 13: 5, 14: 6, 15: 6, 16: 7, 17: 7, 18: 7, 19: 7}\n",
    "barec_5_dict = {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 2, 11: 2, 12: 3, 13: 3, 14: 4, 15: 4, 16: 5, 17: 5, 18: 5, 19: 5}\n",
    "barec_3_dict = {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 2, 13: 2, 14: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3}\n",
    "\n",
    "output_path = \"Test_Sentence_Level.csv\"\n",
    "split = \"Test\"\n",
    "task = \"Sent\"\n",
    "#task_split = task + \"_\" + split\n",
    "task_name = \"Sentence\" if task == \"Sent\" else \"Document\"\n",
    "split_name = \"validation\" if split == \"Dev\" else \"test\"\n",
    "\n",
    "# Placeholder for evaluation logic\n",
    "print(f\"Evaluating {task_name}-level readability on {split} split using {output_path}\")\n",
    "\n",
    "# Collect predictions\n",
    "pred_df = pd.read_csv(output_path, header=0)\n",
    "pred_ids = [str(id) for id in list(pred_df[task_name+ \" ID\"])]\n",
    "predictions = list(pred_df[\"Prediction\"])\n",
    "\n",
    "# Pair ids and predictions in a dictionary\n",
    "pred_dict = dict(zip(pred_ids, predictions))\n",
    "\n",
    "# Get the ground truth data\n",
    "#data_files = {task_split: task_split+\".csv\"}\n",
    "barec = load_dataset(\"CAMeL-Lab/BAREC-Shared-Task-2025-\"+task.lower(), split=split_name)\n",
    "\n",
    "gold_ids = [str(id) for id in list(barec[\"ID\"])]\n",
    "labels = list(barec[\"Readability_Level_19\"])\n",
    "\n",
    "# Check if gold_ids and pred_ids contain exactly the same elements\n",
    "if set(gold_ids) != set(pred_ids):\n",
    "    raise ValueError(\"The output CSV file does not match the split: IDs in predictions and gold data do not align.\")\n",
    "\n",
    "# Align labels and predictions based on gold_ids order\n",
    "aligned_predictions = [pred_dict[id] for id in gold_ids]\n",
    "aligned_labels = labels  # already in gold_ids order\n",
    "\n",
    "# Convert predictions and labels to int for metrics\n",
    "aligned_predictions = [int(p) for p in aligned_predictions]\n",
    "aligned_labels = [int(l) for l in aligned_labels]\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(aligned_labels, aligned_predictions)\n",
    "\n",
    "# Accuracy with margin of 1 level\n",
    "acc_margin_1 = np.mean([abs(p - l) <= 1 for p, l in zip(aligned_predictions, aligned_labels)])\n",
    "\n",
    "# Average absolute distance\n",
    "avg_abs_dist = mean_absolute_error(aligned_labels, aligned_predictions)\n",
    "\n",
    "# Quadratic Cohen's Kappa\n",
    "qwk = cohen_kappa_score(aligned_labels, aligned_predictions, weights='quadratic')\n",
    "\n",
    "# Accuracy for 7, 5, 3 levels\n",
    "aligned_labels_7 = [barec_7_dict[l] for l in aligned_labels]\n",
    "aligned_predictions_7 = [barec_7_dict[p] for p in aligned_predictions]\n",
    "acc_7 = accuracy_score(aligned_labels_7, aligned_predictions_7)\n",
    "\n",
    "aligned_labels_5 = [barec_5_dict[l] for l in aligned_labels]\n",
    "aligned_predictions_5 = [barec_5_dict[p] for p in aligned_predictions]\n",
    "acc_5 = accuracy_score(aligned_labels_5, aligned_predictions_5)\n",
    "\n",
    "aligned_labels_3 = [barec_3_dict[l] for l in aligned_labels]\n",
    "aligned_predictions_3 = [barec_3_dict[p] for p in aligned_predictions]\n",
    "acc_3 = accuracy_score(aligned_labels_3, aligned_predictions_3)\n",
    "\n",
    "print(f\"Accuracy: {acc*100:.4f}%\")\n",
    "print(f\"Accuracy +/-1: {acc_margin_1*100:.4f}%\")\n",
    "print(f\"Average absolute distance: {avg_abs_dist:.6f}\")\n",
    "print(f\"Quadratic Cohen's Kappa: {qwk*100:.4f}%\")\n",
    "print(f\"Accuracy (7 levels): {acc_7*100:.4f}%\")\n",
    "print(f\"Accuracy (5 levels): {acc_5*100:.4f}%\")\n",
    "print(f\"Accuracy (3 levels): {acc_3*100:.4f}%\")\n",
    "print(\"Evaluation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6EPnojPCCGL",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rt5BKz05Adgs"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7867060,
     "sourceId": 12525195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
